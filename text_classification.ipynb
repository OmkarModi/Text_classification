{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "root2ai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQUDbHVvA79deOwPKJscpk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmkarModi/Text_classification/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqAmS4r2mTGi"
      },
      "source": [
        "# Multi Class Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkywPqw3FSSY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKaz6DMjF52F"
      },
      "source": [
        "#paths to various files used in projects\n",
        "\n",
        "data_path = \"/content/root2ai - Data.csv\"\n",
        "word_embedding_path = 'glove.6B.300d.txt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhXAHBadmseI"
      },
      "source": [
        "## data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnF3Doi8GW7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901a7b79-0ae8-464d-a666-7aa96ee7e121"
      },
      "source": [
        "data = pd.read_csv(data_path)\n",
        "print(data.head())\n",
        "print(data.info())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Text      Target\n",
            "0  reserve bank forming expert committee based in...  Blockchain\n",
            "1          director could play role financial system  Blockchain\n",
            "2  preliminary discuss secure transaction study r...  Blockchain\n",
            "3  security indeed prove essential transforming f...  Blockchain\n",
            "4  bank settlement normally take three days based...  Blockchain\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 22704 entries, 0 to 22703\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Text    22701 non-null  object\n",
            " 1   Target  22704 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 354.9+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPOt0qSLpLRO"
      },
      "source": [
        "from info we can find that our data consist some empty or null cells so we need to deal with it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "153jvKt8UP7S"
      },
      "source": [
        "#it is necessary to clean the cells that have NaN values or are empty \n",
        "#so that don't raise errors while performing classification\n",
        "data.dropna(inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKYncCMEobzG"
      },
      "source": [
        "###train test spliting and label encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR4tzL5ootgo"
      },
      "source": [
        "the labels provided are categorial data so it is necessary to encode them so computer could understand them and train it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1AbuIHdG30j"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "X = data['Text']\n",
        "y = data['Target']\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y = np.array(encoder.fit_transform(y))\n",
        "\n",
        "X_train, X_test, y_train , y_test = train_test_split(X,y, test_size = 0.2 , random_state = 0)\n",
        "\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "onehot_train_y = y_train.reshape(len(y_train),1)  #reshaping it to 2d array as OneHotEncoder requires 2d array as perameter\n",
        "onehot_train_y = enc.fit_transform(onehot_train_y)\n",
        "onehot_test_y = y_test.reshape(len(y_test),1)\n",
        "onehot_test_y = enc.fit_transform(onehot_test_y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy3zp9RJqYTk"
      },
      "source": [
        "classes label of our data consists can be obtained. there are 11 classes our labels are distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LqfpCkQmQ0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb227c4-d4c1-423e-e2e5-132a63691c3d"
      },
      "source": [
        "class_names = list(encoder.classes_)\n",
        "print(class_names)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Bigdata', 'Blockchain', 'Cyber Security', 'Data Security', 'FinTech', 'Microservices', 'Neobanks', 'Reg Tech', 'Robo Advising', 'Stock Trading', 'credit reporting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm8CU8ccq2FG"
      },
      "source": [
        "NOTE- In preprocessing step our text needs to be cleaned. we should clean all non word characters, html tags, stopwords and other noises in texts. Data provided to us is already cleaned and is lowercased so this step is skipped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d9COja2rmv_"
      },
      "source": [
        "##Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrdKsGbXruUo"
      },
      "source": [
        "raw text is transformed into meaningful feature vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We3cwUOosV6o"
      },
      "source": [
        "###Count vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfCQ4E4ysmRH"
      },
      "source": [
        "Count Vector is a matrix notation of the dataset in which every row represents a text from the data, every column represents a word from the text, and every cell represents the frequency count of a particular term in a particular document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KsuvJicPwRF"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "count_vect.fit(X_train)\n",
        "count_vect_xtrain = count_vect.transform(X_train)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNv2Gx6ruD0e"
      },
      "source": [
        "Data representation is similar to that of count Vectors but each cell contains a scalar quantity rather than frequency which represents the relative importance of a term in the document "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hWi5cOetKEW"
      },
      "source": [
        "###Word Level Tfid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvhW5O27h2Q9"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "word_tfid_vect = TfidfVectorizer()\n",
        "word_tfid_vect_xtrain = word_tfid_vect.fit_transform(X_train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upaAOHumtlpZ"
      },
      "source": [
        "###ngram Level Tfid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oUqzOoauho3"
      },
      "source": [
        "group of n adjacent words is considered because the group contain important information rather than single word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XpH4YnFtfkl"
      },
      "source": [
        "ngram_tfid_vect = TfidfVectorizer(ngram_range = (2,3))\n",
        "ngram_tfid_vect_xtrain = ngram_tfid_vect.fit_transform(X_train)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XhXq05EtpuO"
      },
      "source": [
        "###Character Level Tfid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7_84j-4u0EB"
      },
      "source": [
        "Here character level score is counted "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFnY1qmltizo"
      },
      "source": [
        "char_tfid_vect = TfidfVectorizer(analyzer = 'char',ngram_range=(2,3))\n",
        "char_tfid_vect_xtrain = char_tfid_vect.fit_transform(X_train)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtLXJkfzvfgW"
      },
      "source": [
        "###Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDmpATE_vlG-"
      },
      "source": [
        "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNK5SJhowc3r",
        "outputId": "e93d1d43-4e34-4908-f8a8-5166b7f751cc"
      },
      "source": [
        "import gensim.models \n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('punkt')\n",
        "sentence = data['Text'].tolist()\n",
        "sent_token = [word_tokenize(sent) for sent in sentence]\n",
        "model = gensim.models.Word2Vec(sentences=sent_token)\n",
        "\n",
        "model.wv.init_sims()\n",
        "\n",
        "#using average vectors is found to be useful feature\n",
        "\n",
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, text) for text in text_list ])\n",
        "\n",
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in sent_tokenize(text, language='english'):\n",
        "        for word in word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "\n",
        "train_tokenized = X_train.apply(w2v_tokenize_text)\n",
        "test_tokenized = X_test.apply(w2v_tokenize_text)\n",
        "\n",
        "X_train_word_average = word_averaging_list(model.wv,train_tokenized)\n",
        "X_test_word_average = word_averaging_list(model.wv,test_tokenized)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPvVRwYH-PeX",
        "outputId": "26419850-4d53-482f-de70-480c4a3caf09"
      },
      "source": [
        "from keras.preprocessing import text, sequence\n",
        "\n",
        "#this maps the word to index \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(data['Text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "#padding sequences to further feed as input to models\n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=50)\n",
        "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=50)\n",
        "\n",
        "#creating embedding matrix that stores vector representation of words\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "  if word in list(model.wv.vocab):\n",
        "    embedding_vector = model[word]\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI-OyR9cw1Jp"
      },
      "source": [
        "###Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxBz1cl6w9tN"
      },
      "source": [
        "This is similar to word2vec but here instead of word whole text is represented to a vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX5bZ4Owf_Tp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a256f364-45c9-4c09-c724-405ab8248872"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn import utils\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re\n",
        "\n",
        "def label_sentences(corpus, label_type):\n",
        "  labeled = []\n",
        "  for i, v in enumerate(corpus):\n",
        "      label = label_type + '_' + str(i)\n",
        "      labeled.append(TaggedDocument(v.split(), [label]))\n",
        "  return labeled\n",
        "\n",
        "X_train_d2v = label_sentences(X_train, 'Train')\n",
        "X_test_d2v = label_sentences(X_test, 'Test')\n",
        "all_data = X_train_d2v + X_test_d2v\n",
        "\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
        "\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha\n",
        "\n",
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    \"\"\"\n",
        "    Get vectors from trained doc2vec model\n",
        "    :param doc2vec_model: Trained Doc2Vec model\n",
        "    :param corpus_size: Size of the data\n",
        "    :param vectors_size: Size of the embedding vectors\n",
        "    :param vectors_type: Training or Testing vectors\n",
        "    :return: list of vectors\n",
        "    \"\"\"\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors\n",
        "    \n",
        "train_vectors_dbow = get_vectors(model_dbow, len(X_train_d2v), 300, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(X_test_d2v), 300, 'Test')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_vectors_dbow = scaler.fit_transform(train_vectors_dbow)\n",
        "test_vectors_dbow = scaler.transform(test_vectors_dbow)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1833629.81it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1458427.46it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2312051.26it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1945066.50it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2779753.46it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2272106.50it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2087954.37it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2705814.18it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2717087.44it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1779119.08it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1739720.36it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2055500.52it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1956014.94it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1916641.07it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2436659.21it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2551925.57it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2259865.07it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2673374.19it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1896673.28it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1576353.35it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2028222.28it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2633300.93it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2059412.88it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2384545.33it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2025978.15it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1940507.78it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1900155.56it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1947373.81it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1525366.38it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 2571707.41it/s]\n",
            "100%|██████████| 22701/22701 [00:00<00:00, 1786228.22it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SnNSal2xXQ6"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss9dXP_dycde"
      },
      "source": [
        "A dictionary that will story evalution matrix for various model and for each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn4jVyBSo66T"
      },
      "source": [
        "model_dict = {}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey3TwTyLxlF-"
      },
      "source": [
        "Function to train and fit various models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t6uYeZ_qMyZ"
      },
      "source": [
        "from sklearn import metrics\n",
        "def model_fit(model,X_train,y_train,X_test,y_test):\n",
        "  classifier = model\n",
        "  classifier.fit(X_train,y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  metric = {'accuracy' : metrics.accuracy_score(y_test,y_pred), 'recall' : metrics.recall_score(y_test,y_pred,average = 'weighted',zero_division=0), 'precision' : metrics.precision_score(y_test,y_pred, average = 'weighted',zero_division=0) , 'f1_score' : metrics.f1_score(y_test, y_pred, average='weighted',zero_division=0) }\n",
        "  return metric"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j--S-UGHxtY2"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWiIamccv-fo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250d25c2-6620-4d13-b690-44973ca86caf"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "LR={}\n",
        "LR['count_vector'] = model_fit(LogisticRegression(max_iter=250),count_vect_xtrain,y_train,count_vect.transform(X_test),y_test)\n",
        "LR['word_tfid'] = model_fit(LogisticRegression(max_iter=250),word_tfid_vect_xtrain,y_train,word_tfid_vect.transform(X_test),y_test)\n",
        "LR['ngram_tfid'] = model_fit(LogisticRegression(max_iter=250),ngram_tfid_vect_xtrain,y_train,ngram_tfid_vect.transform(X_test),y_test)\n",
        "LR['char_tfid'] = model_fit(LogisticRegression(max_iter=250),char_tfid_vect_xtrain,y_train,char_tfid_vect.transform(X_test),y_test)\n",
        "LR['word2v'] = model_fit(LogisticRegression(max_iter=250),X_train_word_average,y_train,X_test_word_average,y_test)\n",
        "LR['doc2v'] = model_fit(LogisticRegression(max_iter=250),train_vectors_dbow,y_train,test_vectors_dbow,y_test)\n",
        "\n",
        "model_dict['LogisticRegression'] = LR"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFYqrsZbkjjy",
        "outputId": "0fa71716-0efd-4b68-fb74-c450a4bc1c69"
      },
      "source": [
        "print(model_dict['LogisticRegression'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'count_vector': {'accuracy': 0.6650517507157014, 'recall': 0.6650517507157014, 'precision': 0.6773009422038596, 'f1_score': 0.6541626476882758}, 'word_tfid': {'accuracy': 0.6538207443294428, 'recall': 0.6538207443294428, 'precision': 0.7002706540977516, 'f1_score': 0.6306351615684646}, 'ngram_tfid': {'accuracy': 0.47148205241136315, 'recall': 0.47148205241136315, 'precision': 0.6986463925421136, 'f1_score': 0.3798440257847721}, 'char_tfid': {'accuracy': 0.6295970050649636, 'recall': 0.6295970050649636, 'precision': 0.6564821187599058, 'f1_score': 0.6034472687215399}, 'word2v': {'accuracy': 0.39374587095353447, 'recall': 0.39374587095353447, 'precision': 0.3196880828752352, 'f1_score': 0.24014897658836998}, 'doc2v': {'accuracy': 0.6362034794098216, 'recall': 0.6362034794098216, 'precision': 0.6297431103283946, 'f1_score': 0.6210494790348875}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0jgPcZgxzRG"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_yFNI001y2h"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "NB={}\n",
        "NB['count_vector'] = model_fit(MultinomialNB(),count_vect_xtrain,y_train,count_vect.transform(X_test),y_test)\n",
        "NB['word_tfid'] = model_fit(MultinomialNB(),word_tfid_vect_xtrain,y_train,word_tfid_vect.transform(X_test),y_test)\n",
        "NB['ngram_tfid'] = model_fit(MultinomialNB(),ngram_tfid_vect_xtrain,y_train,ngram_tfid_vect.transform(X_test),y_test)\n",
        "NB['char_tfid'] = model_fit(MultinomialNB(),char_tfid_vect_xtrain,y_train,char_tfid_vect.transform(X_test),y_test)\n",
        "\n",
        "model_dict['NaiveBayes'] = NB"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb9N0iYCmwlZ",
        "outputId": "5978ee07-b59c-455f-a2cf-bdd38d2fd14d"
      },
      "source": [
        "print(model_dict['NaiveBayes'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'count_vector': {'accuracy': 0.6628495926007487, 'recall': 0.6628495926007487, 'precision': 0.705685421810634, 'f1_score': 0.6347429129700688}, 'word_tfid': {'accuracy': 0.5221316890552742, 'recall': 0.5221316890552742, 'precision': 0.6849210138954444, 'f1_score': 0.4365085440444979}, 'ngram_tfid': {'accuracy': 0.43999119136754017, 'recall': 0.43999119136754017, 'precision': 0.6231222767727924, 'f1_score': 0.32447176194272814}, 'char_tfid': {'accuracy': 0.5091389561770535, 'recall': 0.5091389561770535, 'precision': 0.6514710048615274, 'f1_score': 0.42288955013348173}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT4BJ3OUx7EX"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrCMLSYOF-JU"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "SVM = {}\n",
        "SVM['count_vector'] = model_fit(SGDClassifier(),count_vect_xtrain,y_train,count_vect.transform(X_test),y_test)\n",
        "SVM['word_tfid'] = model_fit(SGDClassifier(),word_tfid_vect_xtrain,y_train,word_tfid_vect.transform(X_test),y_test)\n",
        "SVM['ngram_tfid'] = model_fit(SGDClassifier(),ngram_tfid_vect_xtrain,y_train,ngram_tfid_vect.transform(X_test),y_test)\n",
        "SVM['char_tfid'] = model_fit(SGDClassifier(),char_tfid_vect_xtrain,y_train,char_tfid_vect.transform(X_test),y_test)\n",
        "SVM['word2v'] = model_fit(SGDClassifier(),X_train_word_average,y_train,X_test_word_average,y_test)\n",
        "SVM['doc2v'] = model_fit(SGDClassifier(),train_vectors_dbow,y_train,test_vectors_dbow,y_test)\n",
        "\n",
        "model_dict['SVM'] = SVM"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCFTkv0hnKK4",
        "outputId": "baf7db62-34c9-4dc0-9be7-e5908cec764f"
      },
      "source": [
        "print(model_dict['SVM'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'count_vector': {'accuracy': 0.6643911032812156, 'recall': 0.6643911032812156, 'precision': 0.6607242526621231, 'f1_score': 0.6574610268986137}, 'word_tfid': {'accuracy': 0.6683549878881304, 'recall': 0.6683549878881304, 'precision': 0.6732033958745799, 'f1_score': 0.6497244811533924}, 'ngram_tfid': {'accuracy': 0.5961242017176833, 'recall': 0.5961242017176833, 'precision': 0.6600765616182456, 'f1_score': 0.5699298895413699}, 'char_tfid': {'accuracy': 0.6309182999339352, 'recall': 0.6309182999339352, 'precision': 0.6273450682087197, 'f1_score': 0.6057447617099508}, 'word2v': {'accuracy': 0.383395727813257, 'recall': 0.383395727813257, 'precision': 0.44659473032332553, 'f1_score': 0.28740659727033485}, 'doc2v': {'accuracy': 0.5818101739704911, 'recall': 0.5818101739704911, 'precision': 0.574610093880791, 'f1_score': 0.5709939115322595}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNzz2JWjyKqo"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23_bzluCGsvU"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "RF={}\n",
        "RF['count_vector'] = model_fit(RandomForestClassifier(),count_vect_xtrain,y_train,count_vect.transform(X_test),y_test)\n",
        "RF['word_tfid'] = model_fit(RandomForestClassifier(),word_tfid_vect_xtrain,y_train,word_tfid_vect.transform(X_test),y_test)\n",
        "RF['ngram_tfid'] = model_fit(RandomForestClassifier(),ngram_tfid_vect_xtrain,y_train,ngram_tfid_vect.transform(X_test),y_test)\n",
        "RF['char_tfid'] = model_fit(RandomForestClassifier(),char_tfid_vect_xtrain,y_train,char_tfid_vect.transform(X_test),y_test)\n",
        "RF['word2v'] = model_fit(RandomForestClassifier(),X_train_word_average,y_train,X_test_word_average,y_test)\n",
        "RF['doc2v'] = model_fit(RandomForestClassifier(),train_vectors_dbow,y_train,test_vectors_dbow,y_test)\n",
        "\n",
        "model_dict['RandomForest'] = RF"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaiHTPq1pNbr",
        "outputId": "18fa09d9-fc87-482a-a8b6-0199f6cae9f2"
      },
      "source": [
        "print(model_dict['RandomForest'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'count_vector': {'accuracy': 0.6260735520810394, 'recall': 0.6260735520810394, 'precision': 0.6376836701184349, 'f1_score': 0.6080098236899938}, 'word_tfid': {'accuracy': 0.6432503853776701, 'recall': 0.6432503853776701, 'precision': 0.6767240523003284, 'f1_score': 0.6190746086145794}, 'ngram_tfid': {'accuracy': 0.42105263157894735, 'recall': 0.42105263157894735, 'precision': 0.6236884370924334, 'f1_score': 0.4550963497504706}, 'char_tfid': {'accuracy': 0.5930411803567496, 'recall': 0.5930411803567496, 'precision': 0.6714304898102078, 'f1_score': 0.5435707224797897}, 'word2v': {'accuracy': 0.561770535124422, 'recall': 0.561770535124422, 'precision': 0.6119084781197427, 'f1_score': 0.5220001653072411}, 'doc2v': {'accuracy': 0.5985465756441313, 'recall': 0.5985465756441313, 'precision': 0.640228287539343, 'f1_score': 0.5405410726038635}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyIBczfDyQ3-"
      },
      "source": [
        "### Extreme Gradient Boosting(XGB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Ek1cPfHSdl"
      },
      "source": [
        "import xgboost\n",
        "XGB={}\n",
        "XGB['count_vector'] = model_fit(xgboost.XGBClassifier(),count_vect_xtrain,y_train,count_vect.transform(X_test),y_test)\n",
        "XGB['word_tfid'] = model_fit(xgboost.XGBClassifier(),word_tfid_vect_xtrain,y_train,word_tfid_vect.transform(X_test),y_test)\n",
        "XGB['ngram_tfid'] = model_fit(xgboost.XGBClassifier(),ngram_tfid_vect_xtrain,y_train,ngram_tfid_vect.transform(X_test),y_test)\n",
        "XGB['char_tfid'] = model_fit(xgboost.XGBClassifier(),char_tfid_vect_xtrain,y_train,char_tfid_vect.transform(X_test),y_test)\n",
        "XGB['word2v'] = model_fit(xgboost.XGBClassifier(),X_train_word_average,y_train,X_test_word_average,y_test)\n",
        "XGB['doc2v'] = model_fit(xgboost.XGBClassifier(),train_vectors_dbow,y_train,test_vectors_dbow,y_test)\n",
        "\n",
        "model_dict['XGB'] = XGB"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DraJKau-sxl",
        "outputId": "a4136f1d-01ae-4cad-e430-50593aa5d01b"
      },
      "source": [
        "print(model_dict['XGB'])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'count_vector': {'accuracy': 0.5492182338691918, 'recall': 0.5492182338691918, 'precision': 0.6434636040879212, 'f1_score': 0.5093318825272293}, 'word_tfid': {'accuracy': 0.5516406077956397, 'recall': 0.5516406077956397, 'precision': 0.6570849133628531, 'f1_score': 0.5098584817250998}, 'ngram_tfid': {'accuracy': 0.4487998238273508, 'recall': 0.4487998238273508, 'precision': 0.6957606828452155, 'f1_score': 0.34754921252001186}, 'char_tfid': {'accuracy': 0.5769654261175953, 'recall': 0.5769654261175953, 'precision': 0.6679345422989506, 'f1_score': 0.5304686847030918}, 'word2v': {'accuracy': 0.5265360052851795, 'recall': 0.5265360052851795, 'precision': 0.5430254891197235, 'f1_score': 0.47767916325527116}, 'doc2v': {'accuracy': 0.6161638405637525, 'recall': 0.6161638405637525, 'precision': 0.6244509753435562, 'f1_score': 0.582085846360972}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igOqELpxyxau"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgwzmeT0IlWq"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers, models, optimizers\n",
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size,), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(11, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    return classifier \n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vloIN7TkNiqg"
      },
      "source": [
        "def NN_model(X_train,y_train,X_test,y_test):\n",
        "  classifier = create_model_architecture(X_train.shape[1])\n",
        "  classifier.fit(X_train,y_train,epochs=3)\n",
        "  loss, acc = classifier.evaluate(X_test, onehot_test_y, verbose=0)\n",
        "  print(acc)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  y_pred = y_pred.argmax(axis =-1)\n",
        "  metric = {'accuracy' : metrics.accuracy_score(y_test,y_pred), 'recall' : metrics.recall_score(y_test,y_pred,average = 'weighted'), 'precision' : metrics.precision_score(y_test,y_pred, average = 'weighted') , 'f1_score' : metrics.f1_score(y_test, y_pred, average='macro') }\n",
        "  return metric"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0ms8qPwMfye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5529da3f-9789-4745-c294-3cc33441808d"
      },
      "source": [
        "NN = {}\n",
        "NN['count_vector'] = NN_model(count_vect_xtrain, onehot_train_y,count_vect.transform(X_test),y_test)\n",
        "NN['word_tfid'] = NN_model(word_tfid_vect_xtrain.toarray(),onehot_train_y,word_tfid_vect.transform(X_test).toarray(),y_test)\n",
        "# NN['ngram_tfid'] = NN_model(ngram_tfid_vect_xtrain.toarray(),onehot_train_y,ngram_tfid_vect.transform(X_test).toarray(),y_test)\n",
        "# NN['char_tfid'] = NN_model(char_tfid_vect_xtrain.toarray(),onehot_train_y,char_tfid_vect.transform(X_test).toarray(),y_test)\n",
        "NN['word2v'] = NN_model(X_train_word_average,onehot_train_y,X_test_word_average,y_test)\n",
        "NN['doc2v'] = NN_model(train_vectors_dbow,onehot_train_y,test_vectors_dbow,y_test)\n",
        "\n",
        "model_dict['NN'] = NN"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 100), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "568/568 [==============================] - 21s 15ms/step - loss: 1.7834 - accuracy: 0.4595\n",
            "Epoch 2/3\n",
            "568/568 [==============================] - 8s 14ms/step - loss: 0.7735 - accuracy: 0.7636\n",
            "Epoch 3/3\n",
            "568/568 [==============================] - 8s 15ms/step - loss: 0.5040 - accuracy: 0.8459\n",
            "0.6881744265556335\n",
            "Epoch 1/3\n",
            "568/568 [==============================] - 8s 13ms/step - loss: 1.9421 - accuracy: 0.4090\n",
            "Epoch 2/3\n",
            "568/568 [==============================] - 5s 9ms/step - loss: 0.9358 - accuracy: 0.7173\n",
            "Epoch 3/3\n",
            "568/568 [==============================] - 5s 9ms/step - loss: 0.5994 - accuracy: 0.8138\n",
            "0.6994054317474365\n",
            "Epoch 1/3\n",
            "568/568 [==============================] - 1s 1ms/step - loss: 2.0555 - accuracy: 0.3779\n",
            "Epoch 2/3\n",
            "568/568 [==============================] - 1s 1ms/step - loss: 1.9662 - accuracy: 0.3936\n",
            "Epoch 3/3\n",
            "568/568 [==============================] - 1s 1ms/step - loss: 1.9617 - accuracy: 0.3956\n",
            "0.39396607875823975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "568/568 [==============================] - 1s 1ms/step - loss: 1.4767 - accuracy: 0.5300\n",
            "Epoch 2/3\n",
            "568/568 [==============================] - 1s 2ms/step - loss: 1.0074 - accuracy: 0.6697\n",
            "Epoch 3/3\n",
            "568/568 [==============================] - 1s 1ms/step - loss: 0.9206 - accuracy: 0.6975\n",
            "0.640828013420105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RguXxPa7CwWw",
        "outputId": "4826a01d-793a-4bb6-b451-493955d926c9"
      },
      "source": [
        "print(model_dict['NN'])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'count_vector': {'accuracy': 0.6881744109227043, 'recall': 0.6881744109227043, 'precision': 0.6926543854883103, 'f1_score': 0.6100675512438144}, 'word_tfid': {'accuracy': 0.6994054173089628, 'recall': 0.6994054173089628, 'precision': 0.6978221072207442, 'f1_score': 0.6117355480652417}, 'word2v': {'accuracy': 0.39396608676502975, 'recall': 0.39396608676502975, 'precision': 0.3314684152334756, 'f1_score': 0.07766698730842095}, 'doc2v': {'accuracy': 0.6408280114512221, 'recall': 0.6408280114512221, 'precision': 0.6355252108767004, 'f1_score': 0.5576211221024399}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjuxlN5n-1HF"
      },
      "source": [
        "###Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3offtwNjEuUm"
      },
      "source": [
        "from sklearn import metrics\n",
        "def DNN_model(classify,X_train,y_train,X_test,y_test):\n",
        "  classifier = classify\n",
        "  classifier.fit(X_train,y_train,epochs=60,verbose=2)\n",
        "  loss, acc = classifier.evaluate(test_seq_x, onehot_test_y, verbose=0)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  y_pred = np.argmax(y_pred,axis =-1)\n",
        "  metric = {'accuracy' : metrics.accuracy_score(y_test,y_pred), 'recall' : metrics.recall_score(y_test,y_pred,average = 'weighted'), 'precision' : metrics.precision_score(y_test,y_pred, average = 'weighted') , 'f1_score' : metrics.f1_score(y_test, y_pred, average='macro') }\n",
        "  return metric"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIkY0hpNjB9C",
        "outputId": "4ecf8d79-930c-4bbd-9747-1a98ffc7f528"
      },
      "source": [
        "from keras import layers\n",
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((50, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(11, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_cnn()\n",
        "cnn = {}\n",
        "cnn = DNN_model(classifier,train_seq_x,onehot_train_y,test_seq_x,y_test)\n",
        "model_dict['CNN'] = cnn"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "568/568 - 7s - loss: 1.9905 - accuracy: 0.4015\n",
            "Epoch 2/60\n",
            "568/568 - 5s - loss: 1.9088 - accuracy: 0.4100\n",
            "Epoch 3/60\n",
            "568/568 - 5s - loss: 1.8849 - accuracy: 0.4139\n",
            "Epoch 4/60\n",
            "568/568 - 5s - loss: 1.8569 - accuracy: 0.4236\n",
            "Epoch 5/60\n",
            "568/568 - 5s - loss: 1.8232 - accuracy: 0.4326\n",
            "Epoch 6/60\n",
            "568/568 - 5s - loss: 1.8002 - accuracy: 0.4427\n",
            "Epoch 7/60\n",
            "568/568 - 5s - loss: 1.7820 - accuracy: 0.4475\n",
            "Epoch 8/60\n",
            "568/568 - 5s - loss: 1.7725 - accuracy: 0.4492\n",
            "Epoch 9/60\n",
            "568/568 - 5s - loss: 1.7638 - accuracy: 0.4525\n",
            "Epoch 10/60\n",
            "568/568 - 5s - loss: 1.7570 - accuracy: 0.4522\n",
            "Epoch 11/60\n",
            "568/568 - 5s - loss: 1.7549 - accuracy: 0.4499\n",
            "Epoch 12/60\n",
            "568/568 - 5s - loss: 1.7403 - accuracy: 0.4551\n",
            "Epoch 13/60\n",
            "568/568 - 5s - loss: 1.7464 - accuracy: 0.4516\n",
            "Epoch 14/60\n",
            "568/568 - 5s - loss: 1.7369 - accuracy: 0.4555\n",
            "Epoch 15/60\n",
            "568/568 - 5s - loss: 1.7380 - accuracy: 0.4531\n",
            "Epoch 16/60\n",
            "568/568 - 5s - loss: 1.7387 - accuracy: 0.4537\n",
            "Epoch 17/60\n",
            "568/568 - 5s - loss: 1.7306 - accuracy: 0.4578\n",
            "Epoch 18/60\n",
            "568/568 - 5s - loss: 1.7278 - accuracy: 0.4563\n",
            "Epoch 19/60\n",
            "568/568 - 5s - loss: 1.7222 - accuracy: 0.4586\n",
            "Epoch 20/60\n",
            "568/568 - 5s - loss: 1.7185 - accuracy: 0.4601\n",
            "Epoch 21/60\n",
            "568/568 - 5s - loss: 1.7129 - accuracy: 0.4601\n",
            "Epoch 22/60\n",
            "568/568 - 5s - loss: 1.7131 - accuracy: 0.4579\n",
            "Epoch 23/60\n",
            "568/568 - 5s - loss: 1.7141 - accuracy: 0.4569\n",
            "Epoch 24/60\n",
            "568/568 - 5s - loss: 1.7098 - accuracy: 0.4635\n",
            "Epoch 25/60\n",
            "568/568 - 5s - loss: 1.7032 - accuracy: 0.4637\n",
            "Epoch 26/60\n",
            "568/568 - 5s - loss: 1.6985 - accuracy: 0.4625\n",
            "Epoch 27/60\n",
            "568/568 - 5s - loss: 1.6985 - accuracy: 0.4646\n",
            "Epoch 28/60\n",
            "568/568 - 5s - loss: 1.6986 - accuracy: 0.4616\n",
            "Epoch 29/60\n",
            "568/568 - 5s - loss: 1.6925 - accuracy: 0.4629\n",
            "Epoch 30/60\n",
            "568/568 - 5s - loss: 1.6910 - accuracy: 0.4659\n",
            "Epoch 31/60\n",
            "568/568 - 4s - loss: 1.6915 - accuracy: 0.4639\n",
            "Epoch 32/60\n",
            "568/568 - 5s - loss: 1.6869 - accuracy: 0.4650\n",
            "Epoch 33/60\n",
            "568/568 - 5s - loss: 1.6891 - accuracy: 0.4659\n",
            "Epoch 34/60\n",
            "568/568 - 5s - loss: 1.6829 - accuracy: 0.4657\n",
            "Epoch 35/60\n",
            "568/568 - 5s - loss: 1.6835 - accuracy: 0.4649\n",
            "Epoch 36/60\n",
            "568/568 - 5s - loss: 1.6801 - accuracy: 0.4700\n",
            "Epoch 37/60\n",
            "568/568 - 5s - loss: 1.6828 - accuracy: 0.4681\n",
            "Epoch 38/60\n",
            "568/568 - 5s - loss: 1.6769 - accuracy: 0.4704\n",
            "Epoch 39/60\n",
            "568/568 - 5s - loss: 1.6751 - accuracy: 0.4680\n",
            "Epoch 40/60\n",
            "568/568 - 5s - loss: 1.6701 - accuracy: 0.4702\n",
            "Epoch 41/60\n",
            "568/568 - 5s - loss: 1.6717 - accuracy: 0.4722\n",
            "Epoch 42/60\n",
            "568/568 - 5s - loss: 1.6682 - accuracy: 0.4749\n",
            "Epoch 43/60\n",
            "568/568 - 5s - loss: 1.6716 - accuracy: 0.4686\n",
            "Epoch 44/60\n",
            "568/568 - 5s - loss: 1.6678 - accuracy: 0.4752\n",
            "Epoch 45/60\n",
            "568/568 - 5s - loss: 1.6658 - accuracy: 0.4715\n",
            "Epoch 46/60\n",
            "568/568 - 5s - loss: 1.6665 - accuracy: 0.4725\n",
            "Epoch 47/60\n",
            "568/568 - 5s - loss: 1.6651 - accuracy: 0.4753\n",
            "Epoch 48/60\n",
            "568/568 - 5s - loss: 1.6596 - accuracy: 0.4746\n",
            "Epoch 49/60\n",
            "568/568 - 5s - loss: 1.6581 - accuracy: 0.4753\n",
            "Epoch 50/60\n",
            "568/568 - 5s - loss: 1.6552 - accuracy: 0.4721\n",
            "Epoch 51/60\n",
            "568/568 - 5s - loss: 1.6604 - accuracy: 0.4743\n",
            "Epoch 52/60\n",
            "568/568 - 5s - loss: 1.6545 - accuracy: 0.4743\n",
            "Epoch 53/60\n",
            "568/568 - 5s - loss: 1.6608 - accuracy: 0.4754\n",
            "Epoch 54/60\n",
            "568/568 - 5s - loss: 1.6549 - accuracy: 0.4759\n",
            "Epoch 55/60\n",
            "568/568 - 5s - loss: 1.6542 - accuracy: 0.4770\n",
            "Epoch 56/60\n",
            "568/568 - 5s - loss: 1.6568 - accuracy: 0.4757\n",
            "Epoch 57/60\n",
            "568/568 - 5s - loss: 1.6538 - accuracy: 0.4744\n",
            "Epoch 58/60\n",
            "568/568 - 5s - loss: 1.6540 - accuracy: 0.4767\n",
            "Epoch 59/60\n",
            "568/568 - 5s - loss: 1.6546 - accuracy: 0.4767\n",
            "Epoch 60/60\n",
            "568/568 - 5s - loss: 1.6456 - accuracy: 0.4761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6w0QloFS8lZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03be8edb-aaf9-4fae-eae0-d9bcaddcd73b"
      },
      "source": [
        "print(cnn)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'accuracy': 0.4818321955516406, 'recall': 0.4818321955516406, 'precision': 0.4665161626154696, 'f1_score': 0.2340110028730936}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUA1_VlZYlfG",
        "outputId": "b053802e-b855-4cfd-d62a-39d5f5d0216d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "vocab_size=len(word_index)+1\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=50,trainable=False))\n",
        "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(25, activation='relu'))\n",
        "model.add(Dense(11, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(train_seq_x, onehot_train_y, epochs=30, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(test_seq_x, onehot_test_y, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n",
        "y_pred = model.predict_classes(test_seq_x)\n",
        "\n",
        "\n",
        "cnn2 = {}\n",
        "cnn2['accuracy'] = acc\n",
        "cnn2['recall'] = metrics.recall_score(y_test,y_pred,average = 'weighted')\n",
        "cnn2['precision'] = metrics.precision_score(y_test,y_pred, average = 'weighted')\n",
        "cnn2['f1_score'] = metrics.f1_score(y_test, y_pred, average='macro')\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 50, 100)           1141000   \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 43, 32)            25632     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 21, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 672)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 25)                16825     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 11)                286       \n",
            "=================================================================\n",
            "Total params: 1,183,743\n",
            "Trainable params: 42,743\n",
            "Non-trainable params: 1,141,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "568/568 - 5s - loss: 1.9951 - accuracy: 0.3768\n",
            "Epoch 2/30\n",
            "568/568 - 4s - loss: 1.9041 - accuracy: 0.4014\n",
            "Epoch 3/30\n",
            "568/568 - 4s - loss: 1.8231 - accuracy: 0.4335\n",
            "Epoch 4/30\n",
            "568/568 - 4s - loss: 1.7450 - accuracy: 0.4589\n",
            "Epoch 5/30\n",
            "568/568 - 4s - loss: 1.6786 - accuracy: 0.4771\n",
            "Epoch 6/30\n",
            "568/568 - 4s - loss: 1.6153 - accuracy: 0.4993\n",
            "Epoch 7/30\n",
            "568/568 - 4s - loss: 1.5537 - accuracy: 0.5184\n",
            "Epoch 8/30\n",
            "568/568 - 4s - loss: 1.4949 - accuracy: 0.5330\n",
            "Epoch 9/30\n",
            "568/568 - 4s - loss: 1.4383 - accuracy: 0.5493\n",
            "Epoch 10/30\n",
            "568/568 - 4s - loss: 1.3848 - accuracy: 0.5651\n",
            "Epoch 11/30\n",
            "568/568 - 4s - loss: 1.3374 - accuracy: 0.5828\n",
            "Epoch 12/30\n",
            "568/568 - 4s - loss: 1.2871 - accuracy: 0.5936\n",
            "Epoch 13/30\n",
            "568/568 - 4s - loss: 1.2421 - accuracy: 0.6097\n",
            "Epoch 14/30\n",
            "568/568 - 4s - loss: 1.1986 - accuracy: 0.6244\n",
            "Epoch 15/30\n",
            "568/568 - 4s - loss: 1.1600 - accuracy: 0.6350\n",
            "Epoch 16/30\n",
            "568/568 - 4s - loss: 1.1197 - accuracy: 0.6502\n",
            "Epoch 17/30\n",
            "568/568 - 4s - loss: 1.0881 - accuracy: 0.6580\n",
            "Epoch 18/30\n",
            "568/568 - 4s - loss: 1.0547 - accuracy: 0.6700\n",
            "Epoch 19/30\n",
            "568/568 - 4s - loss: 1.0214 - accuracy: 0.6791\n",
            "Epoch 20/30\n",
            "568/568 - 4s - loss: 0.9916 - accuracy: 0.6907\n",
            "Epoch 21/30\n",
            "568/568 - 4s - loss: 0.9631 - accuracy: 0.7020\n",
            "Epoch 22/30\n",
            "568/568 - 4s - loss: 0.9431 - accuracy: 0.7080\n",
            "Epoch 23/30\n",
            "568/568 - 4s - loss: 0.9178 - accuracy: 0.7151\n",
            "Epoch 24/30\n",
            "568/568 - 4s - loss: 0.8879 - accuracy: 0.7264\n",
            "Epoch 25/30\n",
            "568/568 - 4s - loss: 0.8717 - accuracy: 0.7315\n",
            "Epoch 26/30\n",
            "568/568 - 4s - loss: 0.8514 - accuracy: 0.7362\n",
            "Epoch 27/30\n",
            "568/568 - 4s - loss: 0.8316 - accuracy: 0.7430\n",
            "Epoch 28/30\n",
            "568/568 - 4s - loss: 0.8135 - accuracy: 0.7479\n",
            "Epoch 29/30\n",
            "568/568 - 4s - loss: 0.7904 - accuracy: 0.7569\n",
            "Epoch 30/30\n",
            "568/568 - 4s - loss: 0.7760 - accuracy: 0.7610\n",
            "Test Accuracy: 40.805990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABneoYnn_Qgi"
      },
      "source": [
        "###LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWYh05GmyFNb",
        "outputId": "d9af2d6a-9fac-43ab-a1d4-36437244c406"
      },
      "source": [
        "def create_rnn_lstm():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((50, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(11, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_lstm()\n",
        "lstm = {}\n",
        "lstm = DNN_model(classifier,train_seq_x,onehot_train_y,test_seq_x,y_test)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "568/568 - 25s - loss: 1.9602 - accuracy: 0.4008\n",
            "Epoch 2/60\n",
            "568/568 - 19s - loss: 1.8975 - accuracy: 0.4153\n",
            "Epoch 3/60\n",
            "568/568 - 19s - loss: 1.8767 - accuracy: 0.4173\n",
            "Epoch 4/60\n",
            "568/568 - 19s - loss: 1.8610 - accuracy: 0.4199\n",
            "Epoch 5/60\n",
            "568/568 - 19s - loss: 1.8546 - accuracy: 0.4199\n",
            "Epoch 6/60\n",
            "568/568 - 19s - loss: 1.8359 - accuracy: 0.4267\n",
            "Epoch 7/60\n",
            "568/568 - 19s - loss: 1.8220 - accuracy: 0.4287\n",
            "Epoch 8/60\n",
            "568/568 - 19s - loss: 1.8131 - accuracy: 0.4310\n",
            "Epoch 9/60\n",
            "568/568 - 19s - loss: 1.7920 - accuracy: 0.4393\n",
            "Epoch 10/60\n",
            "568/568 - 19s - loss: 1.7828 - accuracy: 0.4378\n",
            "Epoch 11/60\n",
            "568/568 - 19s - loss: 1.7715 - accuracy: 0.4423\n",
            "Epoch 12/60\n",
            "568/568 - 19s - loss: 1.7536 - accuracy: 0.4492\n",
            "Epoch 13/60\n",
            "568/568 - 19s - loss: 1.7435 - accuracy: 0.4514\n",
            "Epoch 14/60\n",
            "568/568 - 19s - loss: 1.7269 - accuracy: 0.4589\n",
            "Epoch 15/60\n",
            "568/568 - 19s - loss: 1.7185 - accuracy: 0.4590\n",
            "Epoch 16/60\n",
            "568/568 - 19s - loss: 1.7158 - accuracy: 0.4631\n",
            "Epoch 17/60\n",
            "568/568 - 19s - loss: 1.7065 - accuracy: 0.4649\n",
            "Epoch 18/60\n",
            "568/568 - 19s - loss: 1.6993 - accuracy: 0.4649\n",
            "Epoch 19/60\n",
            "568/568 - 19s - loss: 1.6991 - accuracy: 0.4678\n",
            "Epoch 20/60\n",
            "568/568 - 19s - loss: 1.6870 - accuracy: 0.4667\n",
            "Epoch 21/60\n",
            "568/568 - 19s - loss: 1.6810 - accuracy: 0.4705\n",
            "Epoch 22/60\n",
            "568/568 - 19s - loss: 1.6817 - accuracy: 0.4686\n",
            "Epoch 23/60\n",
            "568/568 - 19s - loss: 1.6707 - accuracy: 0.4722\n",
            "Epoch 24/60\n",
            "568/568 - 19s - loss: 1.6603 - accuracy: 0.4764\n",
            "Epoch 25/60\n",
            "568/568 - 19s - loss: 1.6610 - accuracy: 0.4732\n",
            "Epoch 26/60\n",
            "568/568 - 19s - loss: 1.6530 - accuracy: 0.4757\n",
            "Epoch 27/60\n",
            "568/568 - 19s - loss: 1.6496 - accuracy: 0.4752\n",
            "Epoch 28/60\n",
            "568/568 - 19s - loss: 1.6408 - accuracy: 0.4801\n",
            "Epoch 29/60\n",
            "568/568 - 19s - loss: 1.6381 - accuracy: 0.4811\n",
            "Epoch 30/60\n",
            "568/568 - 19s - loss: 1.6306 - accuracy: 0.4816\n",
            "Epoch 31/60\n",
            "568/568 - 19s - loss: 1.6239 - accuracy: 0.4849\n",
            "Epoch 32/60\n",
            "568/568 - 19s - loss: 1.6185 - accuracy: 0.4830\n",
            "Epoch 33/60\n",
            "568/568 - 19s - loss: 1.6193 - accuracy: 0.4822\n",
            "Epoch 34/60\n",
            "568/568 - 19s - loss: 1.6138 - accuracy: 0.4858\n",
            "Epoch 35/60\n",
            "568/568 - 19s - loss: 1.6056 - accuracy: 0.4865\n",
            "Epoch 36/60\n",
            "568/568 - 19s - loss: 1.5912 - accuracy: 0.4906\n",
            "Epoch 37/60\n",
            "568/568 - 19s - loss: 1.5958 - accuracy: 0.4879\n",
            "Epoch 38/60\n",
            "568/568 - 19s - loss: 1.5893 - accuracy: 0.4915\n",
            "Epoch 39/60\n",
            "568/568 - 19s - loss: 1.5874 - accuracy: 0.4917\n",
            "Epoch 40/60\n",
            "568/568 - 19s - loss: 1.5791 - accuracy: 0.4922\n",
            "Epoch 41/60\n",
            "568/568 - 19s - loss: 1.5664 - accuracy: 0.4952\n",
            "Epoch 42/60\n",
            "568/568 - 19s - loss: 1.5596 - accuracy: 0.4986\n",
            "Epoch 43/60\n",
            "568/568 - 19s - loss: 1.5580 - accuracy: 0.4998\n",
            "Epoch 44/60\n",
            "568/568 - 19s - loss: 1.5508 - accuracy: 0.5023\n",
            "Epoch 45/60\n",
            "568/568 - 19s - loss: 1.5422 - accuracy: 0.5035\n",
            "Epoch 46/60\n",
            "568/568 - 19s - loss: 1.5395 - accuracy: 0.5044\n",
            "Epoch 47/60\n",
            "568/568 - 19s - loss: 1.5326 - accuracy: 0.5088\n",
            "Epoch 48/60\n",
            "568/568 - 19s - loss: 1.5277 - accuracy: 0.5062\n",
            "Epoch 49/60\n",
            "568/568 - 19s - loss: 1.5205 - accuracy: 0.5105\n",
            "Epoch 50/60\n",
            "568/568 - 19s - loss: 1.5119 - accuracy: 0.5119\n",
            "Epoch 51/60\n",
            "568/568 - 19s - loss: 1.5138 - accuracy: 0.5101\n",
            "Epoch 52/60\n",
            "568/568 - 19s - loss: 1.5035 - accuracy: 0.5126\n",
            "Epoch 53/60\n",
            "568/568 - 19s - loss: 1.4951 - accuracy: 0.5178\n",
            "Epoch 54/60\n",
            "568/568 - 19s - loss: 1.4860 - accuracy: 0.5221\n",
            "Epoch 55/60\n",
            "568/568 - 19s - loss: 1.4836 - accuracy: 0.5171\n",
            "Epoch 56/60\n",
            "568/568 - 19s - loss: 1.4781 - accuracy: 0.5199\n",
            "Epoch 57/60\n",
            "568/568 - 19s - loss: 1.4771 - accuracy: 0.5216\n",
            "Epoch 58/60\n",
            "568/568 - 19s - loss: 1.4622 - accuracy: 0.5244\n",
            "Epoch 59/60\n",
            "568/568 - 19s - loss: 1.4586 - accuracy: 0.5264\n",
            "Epoch 60/60\n",
            "568/568 - 19s - loss: 1.4584 - accuracy: 0.5261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myLYEP1AGE9r",
        "outputId": "4aee72bc-031d-4a76-b6a0-3418bdf1c8d5"
      },
      "source": [
        "print(lstm['accuracy'])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.48667694340453643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFcrz-lN_YHC"
      },
      "source": [
        "### Gated Recurrent Unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp2hWrWkih3S",
        "outputId": "51d64c21-b727-4310-f814-1d5fb44f5f6a"
      },
      "source": [
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((50, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(11, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics= ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_gru()\n",
        "rnn_gru = {}\n",
        "rnn_gru = DNN_model(classifier,train_seq_x,onehot_train_y,test_seq_x,y_test)\n",
        "wor2v = {}\n",
        "wor2v['word2vec'] = rnn_gru\n",
        "model_dict['GRU'] = wor2v"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "568/568 - 21s - loss: 1.9723 - accuracy: 0.3979\n",
            "Epoch 2/60\n",
            "568/568 - 18s - loss: 1.8921 - accuracy: 0.4158\n",
            "Epoch 3/60\n",
            "568/568 - 18s - loss: 1.8756 - accuracy: 0.4186\n",
            "Epoch 4/60\n",
            "568/568 - 18s - loss: 1.8604 - accuracy: 0.4212\n",
            "Epoch 5/60\n",
            "568/568 - 18s - loss: 1.8397 - accuracy: 0.4225\n",
            "Epoch 6/60\n",
            "568/568 - 18s - loss: 1.8115 - accuracy: 0.4341\n",
            "Epoch 7/60\n",
            "568/568 - 18s - loss: 1.7832 - accuracy: 0.4447\n",
            "Epoch 8/60\n",
            "568/568 - 18s - loss: 1.7621 - accuracy: 0.4520\n",
            "Epoch 9/60\n",
            "568/568 - 18s - loss: 1.7521 - accuracy: 0.4541\n",
            "Epoch 10/60\n",
            "568/568 - 18s - loss: 1.7447 - accuracy: 0.4536\n",
            "Epoch 11/60\n",
            "568/568 - 18s - loss: 1.7334 - accuracy: 0.4572\n",
            "Epoch 12/60\n",
            "568/568 - 18s - loss: 1.7277 - accuracy: 0.4573\n",
            "Epoch 13/60\n",
            "568/568 - 18s - loss: 1.7157 - accuracy: 0.4602\n",
            "Epoch 14/60\n",
            "568/568 - 18s - loss: 1.7099 - accuracy: 0.4623\n",
            "Epoch 15/60\n",
            "568/568 - 18s - loss: 1.7040 - accuracy: 0.4645\n",
            "Epoch 16/60\n",
            "568/568 - 18s - loss: 1.6978 - accuracy: 0.4655\n",
            "Epoch 17/60\n",
            "568/568 - 18s - loss: 1.6875 - accuracy: 0.4672\n",
            "Epoch 18/60\n",
            "568/568 - 18s - loss: 1.6835 - accuracy: 0.4675\n",
            "Epoch 19/60\n",
            "568/568 - 18s - loss: 1.6785 - accuracy: 0.4718\n",
            "Epoch 20/60\n",
            "568/568 - 18s - loss: 1.6724 - accuracy: 0.4718\n",
            "Epoch 21/60\n",
            "568/568 - 18s - loss: 1.6658 - accuracy: 0.4727\n",
            "Epoch 22/60\n",
            "568/568 - 18s - loss: 1.6535 - accuracy: 0.4740\n",
            "Epoch 23/60\n",
            "568/568 - 18s - loss: 1.6466 - accuracy: 0.4784\n",
            "Epoch 24/60\n",
            "568/568 - 18s - loss: 1.6416 - accuracy: 0.4820\n",
            "Epoch 25/60\n",
            "568/568 - 18s - loss: 1.6398 - accuracy: 0.4813\n",
            "Epoch 26/60\n",
            "568/568 - 18s - loss: 1.6313 - accuracy: 0.4828\n",
            "Epoch 27/60\n",
            "568/568 - 18s - loss: 1.6302 - accuracy: 0.4847\n",
            "Epoch 28/60\n",
            "568/568 - 18s - loss: 1.6214 - accuracy: 0.4850\n",
            "Epoch 29/60\n",
            "568/568 - 18s - loss: 1.6108 - accuracy: 0.4883\n",
            "Epoch 30/60\n",
            "568/568 - 18s - loss: 1.6076 - accuracy: 0.4915\n",
            "Epoch 31/60\n",
            "568/568 - 18s - loss: 1.6002 - accuracy: 0.4917\n",
            "Epoch 32/60\n",
            "568/568 - 18s - loss: 1.5954 - accuracy: 0.4909\n",
            "Epoch 33/60\n",
            "568/568 - 18s - loss: 1.5962 - accuracy: 0.4910\n",
            "Epoch 34/60\n",
            "568/568 - 18s - loss: 1.5861 - accuracy: 0.4961\n",
            "Epoch 35/60\n",
            "568/568 - 18s - loss: 1.5794 - accuracy: 0.4969\n",
            "Epoch 36/60\n",
            "568/568 - 18s - loss: 1.5792 - accuracy: 0.4937\n",
            "Epoch 37/60\n",
            "568/568 - 18s - loss: 1.5695 - accuracy: 0.5006\n",
            "Epoch 38/60\n",
            "568/568 - 18s - loss: 1.5641 - accuracy: 0.4991\n",
            "Epoch 39/60\n",
            "568/568 - 18s - loss: 1.5565 - accuracy: 0.5009\n",
            "Epoch 40/60\n",
            "568/568 - 18s - loss: 1.5526 - accuracy: 0.5025\n",
            "Epoch 41/60\n",
            "568/568 - 18s - loss: 1.5437 - accuracy: 0.5046\n",
            "Epoch 42/60\n",
            "568/568 - 18s - loss: 1.5458 - accuracy: 0.5062\n",
            "Epoch 43/60\n",
            "568/568 - 18s - loss: 1.5352 - accuracy: 0.5076\n",
            "Epoch 44/60\n",
            "568/568 - 18s - loss: 1.5287 - accuracy: 0.5128\n",
            "Epoch 45/60\n",
            "568/568 - 18s - loss: 1.5258 - accuracy: 0.5111\n",
            "Epoch 46/60\n",
            "568/568 - 18s - loss: 1.5168 - accuracy: 0.5146\n",
            "Epoch 47/60\n",
            "568/568 - 18s - loss: 1.5095 - accuracy: 0.5164\n",
            "Epoch 48/60\n",
            "568/568 - 18s - loss: 1.5077 - accuracy: 0.5164\n",
            "Epoch 49/60\n",
            "568/568 - 18s - loss: 1.5020 - accuracy: 0.5173\n",
            "Epoch 50/60\n",
            "568/568 - 18s - loss: 1.4922 - accuracy: 0.5194\n",
            "Epoch 51/60\n",
            "568/568 - 18s - loss: 1.4909 - accuracy: 0.5206\n",
            "Epoch 52/60\n",
            "568/568 - 18s - loss: 1.4807 - accuracy: 0.5240\n",
            "Epoch 53/60\n",
            "568/568 - 18s - loss: 1.4727 - accuracy: 0.5255\n",
            "Epoch 54/60\n",
            "568/568 - 18s - loss: 1.4678 - accuracy: 0.5257\n",
            "Epoch 55/60\n",
            "568/568 - 18s - loss: 1.4663 - accuracy: 0.5255\n",
            "Epoch 56/60\n",
            "568/568 - 18s - loss: 1.4606 - accuracy: 0.5279\n",
            "Epoch 57/60\n",
            "568/568 - 18s - loss: 1.4541 - accuracy: 0.5318\n",
            "Epoch 58/60\n",
            "568/568 - 18s - loss: 1.4504 - accuracy: 0.5313\n",
            "Epoch 59/60\n",
            "568/568 - 18s - loss: 1.4473 - accuracy: 0.5350\n",
            "Epoch 60/60\n",
            "568/568 - 18s - loss: 1.4329 - accuracy: 0.5333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYHSJOBT_Tgb"
      },
      "source": [
        "## Evaluating Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arZiZ-CH1d43"
      },
      "source": [
        "temp_model_df = pd.DataFrame(columns=['Model','Feature'])\n",
        "temp_eval_df = pd.DataFrame(columns=['accuracy','recall','precision','f1_score'])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx_8VkCSyGFS",
        "outputId": "5c6fd94b-4208-495e-c3bb-fc0e7a4b68e1"
      },
      "source": [
        "print(model_dict)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LogisticRegression': {'count_vector': {'accuracy': 0.6650517507157014, 'recall': 0.6650517507157014, 'precision': 0.6773009422038596, 'f1_score': 0.6541626476882758}, 'word_tfid': {'accuracy': 0.6538207443294428, 'recall': 0.6538207443294428, 'precision': 0.7002706540977516, 'f1_score': 0.6306351615684646}, 'ngram_tfid': {'accuracy': 0.47148205241136315, 'recall': 0.47148205241136315, 'precision': 0.6986463925421136, 'f1_score': 0.3798440257847721}, 'char_tfid': {'accuracy': 0.6295970050649636, 'recall': 0.6295970050649636, 'precision': 0.6564821187599058, 'f1_score': 0.6034472687215399}, 'word2v': {'accuracy': 0.39374587095353447, 'recall': 0.39374587095353447, 'precision': 0.3196880828752352, 'f1_score': 0.24014897658836998}, 'doc2v': {'accuracy': 0.6362034794098216, 'recall': 0.6362034794098216, 'precision': 0.6297431103283946, 'f1_score': 0.6210494790348875}}, 'NaiveBayes': {'count_vector': {'accuracy': 0.6628495926007487, 'recall': 0.6628495926007487, 'precision': 0.705685421810634, 'f1_score': 0.6347429129700688}, 'word_tfid': {'accuracy': 0.5221316890552742, 'recall': 0.5221316890552742, 'precision': 0.6849210138954444, 'f1_score': 0.4365085440444979}, 'ngram_tfid': {'accuracy': 0.43999119136754017, 'recall': 0.43999119136754017, 'precision': 0.6231222767727924, 'f1_score': 0.32447176194272814}, 'char_tfid': {'accuracy': 0.5091389561770535, 'recall': 0.5091389561770535, 'precision': 0.6514710048615274, 'f1_score': 0.42288955013348173}}, 'SVM': {'count_vector': {'accuracy': 0.6643911032812156, 'recall': 0.6643911032812156, 'precision': 0.6607242526621231, 'f1_score': 0.6574610268986137}, 'word_tfid': {'accuracy': 0.6683549878881304, 'recall': 0.6683549878881304, 'precision': 0.6732033958745799, 'f1_score': 0.6497244811533924}, 'ngram_tfid': {'accuracy': 0.5961242017176833, 'recall': 0.5961242017176833, 'precision': 0.6600765616182456, 'f1_score': 0.5699298895413699}, 'char_tfid': {'accuracy': 0.6309182999339352, 'recall': 0.6309182999339352, 'precision': 0.6273450682087197, 'f1_score': 0.6057447617099508}, 'word2v': {'accuracy': 0.383395727813257, 'recall': 0.383395727813257, 'precision': 0.44659473032332553, 'f1_score': 0.28740659727033485}, 'doc2v': {'accuracy': 0.5818101739704911, 'recall': 0.5818101739704911, 'precision': 0.574610093880791, 'f1_score': 0.5709939115322595}}, 'RandomForest': {'count_vector': {'accuracy': 0.6260735520810394, 'recall': 0.6260735520810394, 'precision': 0.6376836701184349, 'f1_score': 0.6080098236899938}, 'word_tfid': {'accuracy': 0.6432503853776701, 'recall': 0.6432503853776701, 'precision': 0.6767240523003284, 'f1_score': 0.6190746086145794}, 'ngram_tfid': {'accuracy': 0.42105263157894735, 'recall': 0.42105263157894735, 'precision': 0.6236884370924334, 'f1_score': 0.4550963497504706}, 'char_tfid': {'accuracy': 0.5930411803567496, 'recall': 0.5930411803567496, 'precision': 0.6714304898102078, 'f1_score': 0.5435707224797897}, 'word2v': {'accuracy': 0.561770535124422, 'recall': 0.561770535124422, 'precision': 0.6119084781197427, 'f1_score': 0.5220001653072411}, 'doc2v': {'accuracy': 0.5985465756441313, 'recall': 0.5985465756441313, 'precision': 0.640228287539343, 'f1_score': 0.5405410726038635}}, 'XGB': {'count_vector': {'accuracy': 0.5492182338691918, 'recall': 0.5492182338691918, 'precision': 0.6434636040879212, 'f1_score': 0.5093318825272293}, 'word_tfid': {'accuracy': 0.5516406077956397, 'recall': 0.5516406077956397, 'precision': 0.6570849133628531, 'f1_score': 0.5098584817250998}, 'ngram_tfid': {'accuracy': 0.4487998238273508, 'recall': 0.4487998238273508, 'precision': 0.6957606828452155, 'f1_score': 0.34754921252001186}, 'char_tfid': {'accuracy': 0.5769654261175953, 'recall': 0.5769654261175953, 'precision': 0.6679345422989506, 'f1_score': 0.5304686847030918}, 'word2v': {'accuracy': 0.5265360052851795, 'recall': 0.5265360052851795, 'precision': 0.5430254891197235, 'f1_score': 0.47767916325527116}, 'doc2v': {'accuracy': 0.6161638405637525, 'recall': 0.6161638405637525, 'precision': 0.6244509753435562, 'f1_score': 0.582085846360972}}, 'NN': {'count_vector': {'accuracy': 0.6881744109227043, 'recall': 0.6881744109227043, 'precision': 0.6926543854883103, 'f1_score': 0.6100675512438144}, 'word_tfid': {'accuracy': 0.6994054173089628, 'recall': 0.6994054173089628, 'precision': 0.6978221072207442, 'f1_score': 0.6117355480652417}, 'word2v': {'accuracy': 0.39396608676502975, 'recall': 0.39396608676502975, 'precision': 0.3314684152334756, 'f1_score': 0.07766698730842095}, 'doc2v': {'accuracy': 0.6408280114512221, 'recall': 0.6408280114512221, 'precision': 0.6355252108767004, 'f1_score': 0.5576211221024399}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3ykNHkWumGy"
      },
      "source": [
        "model_list = []\n",
        "feature_list = []\n",
        "\n",
        "for  mod,feature_dict in model_dict.items():\n",
        "  for feature,evaluation_dict in feature_dict.items():\n",
        "    model_list.append(mod)\n",
        "    feature_list.append(feature)\n",
        "    temp_eval_df = temp_eval_df.append(evaluation_dict,ignore_index=True)\n",
        "temp_model_df['Model'] = model_list\n",
        "temp_model_df['Feature'] = feature_list"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU9hND_m4BiA",
        "outputId": "cf89ef6a-401e-4755-dd5c-bda232b2f712"
      },
      "source": [
        "eval_df=pd.concat([temp_model_df,temp_eval_df],axis=1)\n",
        "print(eval_df)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 Model       Feature  accuracy    recall  precision  f1_score\n",
            "0   LogisticRegression  count_vector  0.665052  0.665052   0.677301  0.654163\n",
            "1   LogisticRegression     word_tfid  0.653821  0.653821   0.700271  0.630635\n",
            "2   LogisticRegression    ngram_tfid  0.471482  0.471482   0.698646  0.379844\n",
            "3   LogisticRegression     char_tfid  0.629597  0.629597   0.656482  0.603447\n",
            "4   LogisticRegression        word2v  0.393746  0.393746   0.319688  0.240149\n",
            "5   LogisticRegression         doc2v  0.636203  0.636203   0.629743  0.621049\n",
            "6           NaiveBayes  count_vector  0.662850  0.662850   0.705685  0.634743\n",
            "7           NaiveBayes     word_tfid  0.522132  0.522132   0.684921  0.436509\n",
            "8           NaiveBayes    ngram_tfid  0.439991  0.439991   0.623122  0.324472\n",
            "9           NaiveBayes     char_tfid  0.509139  0.509139   0.651471  0.422890\n",
            "10                 SVM  count_vector  0.664391  0.664391   0.660724  0.657461\n",
            "11                 SVM     word_tfid  0.668355  0.668355   0.673203  0.649724\n",
            "12                 SVM    ngram_tfid  0.596124  0.596124   0.660077  0.569930\n",
            "13                 SVM     char_tfid  0.630918  0.630918   0.627345  0.605745\n",
            "14                 SVM        word2v  0.383396  0.383396   0.446595  0.287407\n",
            "15                 SVM         doc2v  0.581810  0.581810   0.574610  0.570994\n",
            "16        RandomForest  count_vector  0.626074  0.626074   0.637684  0.608010\n",
            "17        RandomForest     word_tfid  0.643250  0.643250   0.676724  0.619075\n",
            "18        RandomForest    ngram_tfid  0.421053  0.421053   0.623688  0.455096\n",
            "19        RandomForest     char_tfid  0.593041  0.593041   0.671430  0.543571\n",
            "20        RandomForest        word2v  0.561771  0.561771   0.611908  0.522000\n",
            "21        RandomForest         doc2v  0.598547  0.598547   0.640228  0.540541\n",
            "22                 XGB  count_vector  0.549218  0.549218   0.643464  0.509332\n",
            "23                 XGB     word_tfid  0.551641  0.551641   0.657085  0.509858\n",
            "24                 XGB    ngram_tfid  0.448800  0.448800   0.695761  0.347549\n",
            "25                 XGB     char_tfid  0.576965  0.576965   0.667935  0.530469\n",
            "26                 XGB        word2v  0.526536  0.526536   0.543025  0.477679\n",
            "27                 XGB         doc2v  0.616164  0.616164   0.624451  0.582086\n",
            "28                  NN  count_vector  0.688174  0.688174   0.692654  0.610068\n",
            "29                  NN     word_tfid  0.699405  0.699405   0.697822  0.611736\n",
            "30                  NN        word2v  0.393966  0.393966   0.331468  0.077667\n",
            "31                  NN         doc2v  0.640828  0.640828   0.635525  0.557621\n",
            "32                 CNN      word2vec  0.481832  0.481832   0.466516  0.234011\n",
            "33                CNN2      word2vec  0.408060  0.408280   0.391929  0.271799\n",
            "34                LSTM      word2vec  0.486677  0.486677   0.480852  0.274471\n",
            "35                 GRU      word2vec  0.492182  0.492182   0.464086  0.296559\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}