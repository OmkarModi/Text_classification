{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "root2ai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkIpHzGN6CdFUPZQsZIQfw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmkarModi/Text_classification/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqAmS4r2mTGi"
      },
      "source": [
        "# Multi Class Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkywPqw3FSSY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKaz6DMjF52F"
      },
      "source": [
        "#paths to various files used in projects\n",
        "\n",
        "data_path = \"/content/root2ai - Data.csv\"\n",
        "word_embedding_path = 'glove.6B.300d.txt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhXAHBadmseI"
      },
      "source": [
        "## data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnF3Doi8GW7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00686b20-3b59-4438-a9ae-44b5f053abf5"
      },
      "source": [
        "data = pd.read_csv(data_path)\n",
        "print(data.head())\n",
        "print(data.info())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Text      Target\n",
            "0  reserve bank forming expert committee based in...  Blockchain\n",
            "1          director could play role financial system  Blockchain\n",
            "2  preliminary discuss secure transaction study r...  Blockchain\n",
            "3  security indeed prove essential transforming f...  Blockchain\n",
            "4  bank settlement normally take three days based...  Blockchain\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 22704 entries, 0 to 22703\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Text    22701 non-null  object\n",
            " 1   Target  22704 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 354.9+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPOt0qSLpLRO"
      },
      "source": [
        "from info we can find that our data consist some empty or null cells so we need to deal with it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "153jvKt8UP7S"
      },
      "source": [
        "#it is necessary to clean the cells that have NaN values or are empty \n",
        "#so that don't raise errors while performing classification\n",
        "data.dropna(inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKYncCMEobzG"
      },
      "source": [
        "###train test spliting and label encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR4tzL5ootgo"
      },
      "source": [
        "the labels provided are categorial data so it is necessary to encode them so computer could understand them and train it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1AbuIHdG30j"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "X = data['Text']\n",
        "y = data['Target']\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y = np.array(encoder.fit_transform(y))\n",
        "\n",
        "X_train, X_test, y_train , y_test = train_test_split(X,y, test_size = 0.2 , random_state = 0)\n",
        "\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "onehot_train_y = y_train.reshape(len(y_train),1)  #reshaping it to 2d array as OneHotEncoder requires 2d array as perameter\n",
        "onehot_train_y = enc.fit_transform(onehot_train_y)\n",
        "onehot_test_y = y_test.reshape(len(y_test),1)\n",
        "onehot_test_y = enc.fit_transform(onehot_test_y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy3zp9RJqYTk"
      },
      "source": [
        "classes label of our data consists can be obtained. there are 11 classes our labels are distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LqfpCkQmQ0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cfa0a08-bd35-453a-dc54-f0052ae70a87"
      },
      "source": [
        "class_names = list(encoder.classes_)\n",
        "print(class_names)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Bigdata', 'Blockchain', 'Cyber Security', 'Data Security', 'FinTech', 'Microservices', 'Neobanks', 'Reg Tech', 'Robo Advising', 'Stock Trading', 'credit reporting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm8CU8ccq2FG"
      },
      "source": [
        "NOTE- In preprocessing step our text needs to be cleaned. we should clean all non word characters, html tags, stopwords and other noises in texts. Data provided to us is already cleaned and is lowercased so this step is skipped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d9COja2rmv_"
      },
      "source": [
        "##Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrdKsGbXruUo"
      },
      "source": [
        "raw text is transformed into meaningful feature vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We3cwUOosV6o"
      },
      "source": [
        "###Count vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfCQ4E4ysmRH"
      },
      "source": [
        "Count Vector is a matrix notation of the dataset in which every row represents a text from the data, every column represents a word from the text, and every cell represents the frequency count of a particular term in a particular document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KsuvJicPwRF"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "count_vect.fit(X_train)\n",
        "count_vect_xtrain = count_vect.transform(X_train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNv2Gx6ruD0e"
      },
      "source": [
        "Data representation is similar to that of count Vectors but each cell contains a scalar quantity rather than frequency which represents the relative importance of a term in the document "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hWi5cOetKEW"
      },
      "source": [
        "###Word Level Tfid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvhW5O27h2Q9"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "word_tfid_vect = TfidfVectorizer()\n",
        "word_tfid_vect_xtrain = word_tfid_vect.fit_transform(X_train)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upaAOHumtlpZ"
      },
      "source": [
        "###ngram Level Tfid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oUqzOoauho3"
      },
      "source": [
        "group of n adjacent words is considered because the group contain important information rather than single word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XpH4YnFtfkl"
      },
      "source": [
        "ngram_tfid_vect = TfidfVectorizer(ngram_range = (2,3))\n",
        "ngram_tfid_vect_xtrain = ngram_tfid_vect.fit_transform(X_train)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XhXq05EtpuO"
      },
      "source": [
        "###Character Level Tfid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7_84j-4u0EB"
      },
      "source": [
        "Here character level score is counted "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFnY1qmltizo"
      },
      "source": [
        "char_tfid_vect = TfidfVectorizer(analyzer = 'char',ngram_range=(2,3))\n",
        "char_tfid_vect_xtrain = char_tfid_vect.fit_transform(X_train)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtLXJkfzvfgW"
      },
      "source": [
        "###Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDmpATE_vlG-"
      },
      "source": [
        "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNK5SJhowc3r",
        "outputId": "57531cee-f5ff-4b5a-b828-fb5df140b3dc"
      },
      "source": [
        "import gensim.models \n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('punkt')\n",
        "sentence = data['Text'].tolist()\n",
        "sent_token = [word_tokenize(sent) for sent in sentence]\n",
        "model = gensim.models.Word2Vec(sentences=sent_token)\n",
        "\n",
        "model.wv.init_sims()\n",
        "\n",
        "#using average vectors is found to be useful feature\n",
        "\n",
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, text) for text in text_list ])\n",
        "\n",
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in sent_tokenize(text, language='english'):\n",
        "        for word in word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "\n",
        "train_tokenized = X_train.apply(w2v_tokenize_text)\n",
        "test_tokenized = X_test.apply(w2v_tokenize_text)\n",
        "\n",
        "X_train_word_average = word_averaging_list(model.wv,train_tokenized)\n",
        "X_test_word_average = word_averaging_list(model.wv,test_tokenized)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPvVRwYH-PeX",
        "outputId": "3eec1c1f-9602-43ba-d0a6-3c230506eb99"
      },
      "source": [
        "from keras.preprocessing import text, sequence\n",
        "\n",
        "#this maps the word to index \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(data['Text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "#padding sequences to further feed as input to models\n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=50)\n",
        "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=50)\n",
        "\n",
        "#creating embedding matrix that stores vector representation of words\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "  if word in list(model.wv.vocab):\n",
        "    embedding_vector = model[word]\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI-OyR9cw1Jp"
      },
      "source": [
        "###Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxBz1cl6w9tN"
      },
      "source": [
        "This is similar to word2vec but here instead of word whole text is represented to a vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX5bZ4Owf_Tp"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn import utils\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re\n",
        "\n",
        "def label_sentences(corpus, label_type):\n",
        "  labeled = []\n",
        "  for i, v in enumerate(corpus):\n",
        "      label = label_type + '_' + str(i)\n",
        "      labeled.append(TaggedDocument(v.split(), [label]))\n",
        "  return labeled\n",
        "\n",
        "X_train_d2v = label_sentences(X_train, 'Train')\n",
        "X_test_d2v = label_sentences(X_test, 'Test')\n",
        "all_data = X_train_d2v + X_test_d2v\n",
        "\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
        "\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha\n",
        "\n",
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    \"\"\"\n",
        "    Get vectors from trained doc2vec model\n",
        "    :param doc2vec_model: Trained Doc2Vec model\n",
        "    :param corpus_size: Size of the data\n",
        "    :param vectors_size: Size of the embedding vectors\n",
        "    :param vectors_type: Training or Testing vectors\n",
        "    :return: list of vectors\n",
        "    \"\"\"\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors\n",
        "    \n",
        "train_vectors_dbow = get_vectors(model_dbow, len(X_train_d2v), 300, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(X_test_d2v), 300, 'Test')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_vectors_dbow = scaler.fit_transform(train_vectors_dbow)\n",
        "test_vectors_dbow = scaler.transform(test_vectors_dbow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SnNSal2xXQ6"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss9dXP_dycde"
      },
      "source": [
        "A dictionary that will story evalution matrix for various model and for each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn4jVyBSo66T"
      },
      "source": [
        "model = {}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey3TwTyLxlF-"
      },
      "source": [
        "Function to train and fit various models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t6uYeZ_qMyZ"
      },
      "source": [
        "from sklearn import metrics\n",
        "def model_fit(model,X_train,y_train,X_test,y_test):\n",
        "  classifier = model\n",
        "  classifier.fit(X_train,y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  metric = {'accuracy' : metrics.accuracy_score(y_test,y_pred), 'recall' : metrics.recall_score(y_test,y_pred,average = 'weighted',zero_division=0), 'precision' : metrics.precision_score(y_test,y_pred, average = 'weighted',zero_division=0) , 'f1_score' : metrics.f1_score(y_test, y_pred, average='macro',zero_division=0) }\n",
        "  return metric"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j--S-UGHxtY2"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWiIamccv-fo"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "LR={}\n",
        "LR['count_vector'] = model_fit(LogisticRegression(max_iter=250),count_vect_xtrain,y_train,count_vect.transform(X_test),y_test)\n",
        "LR['word_tfid'] = model_fit(LogisticRegression(max_iter=250),word_tfid_vect_xtrain,y_train,word_tfid_vect.transform(X_test),y_test)\n",
        "LR['ngram_tfid'] = model_fit(LogisticRegression(max_iter=250),ngram_tfid_vect_xtrain,y_train,ngram_tfid_vect.transform(X_test),y_test)\n",
        "LR['char_tfid'] = model_fit(LogisticRegression(max_iter=250),char_tfid_vect_xtrain,y_train,char_tfid_vect.transform(X_test),y_test)\n",
        "LR['word2v'] = model_fit(LogisticRegression(max_iter=250),X_train_word_average,y_train,X_test_word_average,y_test)\n",
        "LR['doc2v'] = model_fit(LogisticRegression(max_iter=250),train_vectors_dbow,y_train,test_vectors_dbow,y_test)\n",
        "\n",
        "model['LogisticRegression'] = LR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0jgPcZgxzRG"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_yFNI001y2h"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "NB={}\n",
        "NB['count_vector'] = model_fit(MultinomialNB(),count_vect_xtrain,y_train,count_vect.transform(X_test),y_test)\n",
        "NB['word_tfid'] = model_fit(MultinomialNB(),word_tfid_vect_xtrain,y_train,word_tfid_vect.transform(X_test),y_test)\n",
        "NB['ngram_tfid'] = model_fit(MultinomialNB(),ngram_tfid_vect_xtrain,y_train,ngram_tfid_vect.transform(X_test),y_test)\n",
        "NB['char_tfid'] = model_fit(MultinomialNB(),char_tfid_vect_xtrain,y_train,char_tfid_vect.transform(X_test),y_test)\n",
        "\n",
        "model['NaiveBayes'] = NB"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT4BJ3OUx7EX"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrCMLSYOF-JU"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "SVM = {}\n",
        "SVM['count_vector'] = model_fit(SGDClassifier(),count_vect_xtrain,y_train,count_vect.transform(X_test),y_test)\n",
        "SVM['word_tfid'] = model_fit(SGDClassifier(),word_tfid_vect_xtrain,y_train,word_tfid_vect.transform(X_test),y_test)\n",
        "SVM['ngram_tfid'] = model_fit(SGDClassifier(),ngram_tfid_vect_xtrain,y_train,ngram_tfid_vect.transform(X_test),y_test)\n",
        "SVM['char_tfid'] = model_fit(SGDClassifier(),char_tfid_vect_xtrain,y_train,char_tfid_vect.transform(X_test),y_test)\n",
        "SVM['word2v'] = model_fit(SGDClassifier(),X_train_word_average,y_train,X_test_word_average,y_test)\n",
        "SVM['doc2v'] = model_fit(SGDClassifier(),train_vectors_dbow,y_train,test_vectors_dbow,y_test)\n",
        "\n",
        "model['SVM'] = SVM"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNzz2JWjyKqo"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23_bzluCGsvU"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "RF={}\n",
        "RF['count_vector'] = model_fit(RandomForestClassifier(),count_vect_xtrain,y_train,count_vect.transform(X_test),y_test)\n",
        "RF['word_tfid'] = model_fit(RandomForestClassifier(),word_tfid_vect_xtrain,y_train,word_tfid_vect.transform(X_test),y_test)\n",
        "RF['ngram_tfid'] = model_fit(RandomForestClassifier(),ngram_tfid_vect_xtrain,y_train,ngram_tfid_vect.transform(X_test),y_test)\n",
        "RF['char_tfid'] = model_fit(RandomForestClassifier(),char_tfid_vect_xtrain,y_train,char_tfid_vect.transform(X_test),y_test)\n",
        "RF['word2v'] = model_fit(RandomForestClassifier(),X_train_word_average,y_train,X_test_word_average,y_test)\n",
        "RF['doc2v'] = model_fit(RandomForestClassifier(),train_vectors_dbow,y_train,test_vectors_dbow,y_test)\n",
        "\n",
        "model['RandomForest'] = RF"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyIBczfDyQ3-"
      },
      "source": [
        "### Extreme Gradient Boosting(XGB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Ek1cPfHSdl"
      },
      "source": [
        "import xgboost\n",
        "XGB={}\n",
        "XGB['count_vector'] = model_fit(xgboost.XGBClassifier(),count_vect_xtrain,y_train,count_vect.transform(X_test),y_test)\n",
        "XGB['word_tfid'] = model_fit(xgboost.XGBClassifier(),word_tfid_vect_xtrain,y_train,word_tfid_vect.transform(X_test),y_test)\n",
        "XGB['ngram_tfid'] = model_fit(xgboost.XGBClassifier(),ngram_tfid_vect_xtrain,y_train,ngram_tfid_vect.transform(X_test),y_test)\n",
        "XGB['char_tfid'] = model_fit(xgboost.XGBClassifier(),char_tfid_vect_xtrain,y_train,char_tfid_vect.transform(X_test),y_test)\n",
        "XGB['word2v'] = model_fit(xgboost.XGBClassifier(),X_train_word_average,y_train,X_test_word_average,y_test)\n",
        "XGB['doc2v'] = model_fit(xgboost.XGBClassifier(),train_vectors_dbow,y_train,test_vectors_dbow,y_test)\n",
        "\n",
        "model['XGB'] = XGB"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igOqELpxyxau"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgwzmeT0IlWq"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers, models, optimizers\n",
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size,), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(11, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    print(classifier.summary())\n",
        "    return classifier \n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vloIN7TkNiqg"
      },
      "source": [
        "def NN_model(X_train,y_train,X_test,y_test):\n",
        "  classifier = create_model_architecture(X_train.shape[1])\n",
        "  classifier.fit(X_train,y_train,epochs=25)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  y_pred = y_pred.argmax(axis =-1)\n",
        "  metric = {'accuracy' : metrics.accuracy_score(y_test,y_pred), 'recall' : metrics.recall_score(y_test,y_pred,average = 'weighted'), 'precision' : metrics.precision_score(y_test,y_pred, average = 'weighted') , 'f1_score' : metrics.f1_score(y_test, y_pred, average='macro') }\n",
        "  return metric"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ms8qPwMfye",
        "outputId": "b12dd1b9-afb8-492e-a376-d65e688514a3"
      },
      "source": [
        "NN = {}\n",
        "NN['count_vector'] = NN_model(count_vect_xtrain, onehot_train_y,count_vect.transform(X_test),y_test)\n",
        "NN['word_tfid'] = NN_model(word_tfid_vect_xtrain.toarray(),onehot_train_y,word_tfid_vect.transform(X_test).toarray(),y_test)\n",
        "NN['ngram_tfid'] = NN_model(ngram_tfid_vect_xtrain.toarray(),onehot_train_y,ngram_tfid_vect.transform(X_test).toarray(),y_test)\n",
        "NN['char_tfid'] = NN_model(char_tfid_vect_xtrain.toarray(),onehot_train_y,char_tfid_vect.transform(X_test).toarray(),y_test)\n",
        "NN['word2v'] = NN_model(X_train_word_average,onehot_train_y,X_test_word_average,y_test)\n",
        "NN['doc2v'] = NN_model(train_vectors_dbow,onehot_train_y,test_vectors_dbow,y_test)\n",
        "\n",
        "model['NN'] = NN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 10506)]           0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 100)               1050700   \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 11)                1111      \n",
            "=================================================================\n",
            "Total params: 1,051,811\n",
            "Trainable params: 1,051,811\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_7/dense_18/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_7/dense_18/embedding_lookup_sparse/Reshape:0\", shape=(None, 100), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_7/dense_18/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "568/568 [==============================] - 10s 15ms/step - loss: 1.7809 - accuracy: 0.4569\n",
            "Epoch 2/25\n",
            "568/568 [==============================] - 9s 15ms/step - loss: 0.7699 - accuracy: 0.7625\n",
            "Epoch 3/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.4962 - accuracy: 0.8513\n",
            "Epoch 4/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.3585 - accuracy: 0.8917\n",
            "Epoch 5/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.2750 - accuracy: 0.9195\n",
            "Epoch 6/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.2212 - accuracy: 0.9343\n",
            "Epoch 7/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.1808 - accuracy: 0.9485\n",
            "Epoch 8/25\n",
            "568/568 [==============================] - 9s 17ms/step - loss: 0.1545 - accuracy: 0.9540\n",
            "Epoch 9/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.1279 - accuracy: 0.9622\n",
            "Epoch 10/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.1085 - accuracy: 0.9674\n",
            "Epoch 11/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.0934 - accuracy: 0.9729\n",
            "Epoch 12/25\n",
            "568/568 [==============================] - 9s 15ms/step - loss: 0.0860 - accuracy: 0.9720\n",
            "Epoch 13/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.0769 - accuracy: 0.9762\n",
            "Epoch 14/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.0637 - accuracy: 0.9813\n",
            "Epoch 15/25\n",
            "568/568 [==============================] - 9s 15ms/step - loss: 0.0619 - accuracy: 0.9819\n",
            "Epoch 16/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.0534 - accuracy: 0.9832\n",
            "Epoch 17/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.0503 - accuracy: 0.9833\n",
            "Epoch 18/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.0473 - accuracy: 0.9846\n",
            "Epoch 19/25\n",
            "568/568 [==============================] - 9s 15ms/step - loss: 0.0461 - accuracy: 0.9843\n",
            "Epoch 20/25\n",
            "568/568 [==============================] - 9s 15ms/step - loss: 0.0375 - accuracy: 0.9875\n",
            "Epoch 21/25\n",
            "568/568 [==============================] - 9s 15ms/step - loss: 0.0357 - accuracy: 0.9876\n",
            "Epoch 22/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.0343 - accuracy: 0.9882\n",
            "Epoch 23/25\n",
            "568/568 [==============================] - 9s 15ms/step - loss: 0.0329 - accuracy: 0.9875\n",
            "Epoch 24/25\n",
            "568/568 [==============================] - 9s 15ms/step - loss: 0.0324 - accuracy: 0.9876\n",
            "Epoch 25/25\n",
            "568/568 [==============================] - 9s 16ms/step - loss: 0.0286 - accuracy: 0.9891\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 10506)]           0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 100)               1050700   \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 11)                1111      \n",
            "=================================================================\n",
            "Total params: 1,051,811\n",
            "Trainable params: 1,051,811\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 1.9447 - accuracy: 0.4106\n",
            "Epoch 2/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.9446 - accuracy: 0.7158\n",
            "Epoch 3/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.5981 - accuracy: 0.8149\n",
            "Epoch 4/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.4418 - accuracy: 0.8651\n",
            "Epoch 5/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.3531 - accuracy: 0.8937\n",
            "Epoch 6/25\n",
            "568/568 [==============================] - 5s 10ms/step - loss: 0.2944 - accuracy: 0.9114\n",
            "Epoch 7/25\n",
            "568/568 [==============================] - 5s 10ms/step - loss: 0.2434 - accuracy: 0.9303\n",
            "Epoch 8/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.2004 - accuracy: 0.9423\n",
            "Epoch 9/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.1760 - accuracy: 0.9517\n",
            "Epoch 10/25\n",
            "568/568 [==============================] - 5s 10ms/step - loss: 0.1647 - accuracy: 0.9520\n",
            "Epoch 11/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.1412 - accuracy: 0.9603\n",
            "Epoch 12/25\n",
            "568/568 [==============================] - 5s 10ms/step - loss: 0.1242 - accuracy: 0.9664\n",
            "Epoch 13/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.1086 - accuracy: 0.9699\n",
            "Epoch 14/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0979 - accuracy: 0.9751\n",
            "Epoch 15/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0870 - accuracy: 0.9772\n",
            "Epoch 16/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0852 - accuracy: 0.9756\n",
            "Epoch 17/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0780 - accuracy: 0.9768\n",
            "Epoch 18/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0668 - accuracy: 0.9806\n",
            "Epoch 19/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0624 - accuracy: 0.9815\n",
            "Epoch 20/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0581 - accuracy: 0.9832\n",
            "Epoch 21/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0569 - accuracy: 0.9827\n",
            "Epoch 22/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0514 - accuracy: 0.9828\n",
            "Epoch 23/25\n",
            "568/568 [==============================] - 5s 10ms/step - loss: 0.0468 - accuracy: 0.9843\n",
            "Epoch 24/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0422 - accuracy: 0.9870\n",
            "Epoch 25/25\n",
            "568/568 [==============================] - 6s 10ms/step - loss: 0.0423 - accuracy: 0.9852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3offtwNjEuUm"
      },
      "source": [
        "from sklearn import metrics\n",
        "def DNN_model(classify,X_train,y_train,X_test,y_test):\n",
        "  classifier = classify\n",
        "  classifier.fit(X_train,y_train,epochs=100,verbose=2)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  y_pred = np.argmax(y_pred,axis =-1)\n",
        "  metric = {'accuracy' : metrics.accuracy_score(y_test,y_pred), 'recall' : metrics.recall_score(y_test,y_pred,average = 'weighted'), 'precision' : metrics.precision_score(y_test,y_pred, average = 'weighted') , 'f1_score' : metrics.f1_score(y_test, y_pred, average='macro') }\n",
        "  return metric"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjuxlN5n-1HF"
      },
      "source": [
        "###Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIkY0hpNjB9C",
        "outputId": "0b8d24f7-aeaf-4232-94d0-57ecc64ac880"
      },
      "source": [
        "from keras import layers\n",
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((50, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(11, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_cnn()\n",
        "cnn = {}\n",
        "cnn = DNN_model(classifier,train_seq_x,onehot_train_y,test_seq_x,y_test)\n",
        "model['CNN'] = cnn"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "568/568 - 7s - loss: 1.9808 - accuracy: 0.3964\n",
            "Epoch 2/100\n",
            "568/568 - 6s - loss: 1.8993 - accuracy: 0.4117\n",
            "Epoch 3/100\n",
            "568/568 - 7s - loss: 1.8744 - accuracy: 0.4183\n",
            "Epoch 4/100\n",
            "568/568 - 9s - loss: 1.8505 - accuracy: 0.4263\n",
            "Epoch 5/100\n",
            "568/568 - 9s - loss: 1.8380 - accuracy: 0.4284\n",
            "Epoch 6/100\n",
            "568/568 - 9s - loss: 1.8193 - accuracy: 0.4308\n",
            "Epoch 7/100\n",
            "568/568 - 9s - loss: 1.8125 - accuracy: 0.4314\n",
            "Epoch 8/100\n",
            "568/568 - 10s - loss: 1.7977 - accuracy: 0.4351\n",
            "Epoch 9/100\n",
            "568/568 - 10s - loss: 1.7870 - accuracy: 0.4386\n",
            "Epoch 10/100\n",
            "568/568 - 10s - loss: 1.7737 - accuracy: 0.4470\n",
            "Epoch 11/100\n",
            "568/568 - 10s - loss: 1.7702 - accuracy: 0.4473\n",
            "Epoch 12/100\n",
            "568/568 - 10s - loss: 1.7566 - accuracy: 0.4472\n",
            "Epoch 13/100\n",
            "568/568 - 10s - loss: 1.7536 - accuracy: 0.4494\n",
            "Epoch 14/100\n",
            "568/568 - 10s - loss: 1.7464 - accuracy: 0.4503\n",
            "Epoch 15/100\n",
            "568/568 - 10s - loss: 1.7455 - accuracy: 0.4487\n",
            "Epoch 16/100\n",
            "568/568 - 10s - loss: 1.7407 - accuracy: 0.4496\n",
            "Epoch 17/100\n",
            "568/568 - 10s - loss: 1.7386 - accuracy: 0.4524\n",
            "Epoch 18/100\n",
            "568/568 - 10s - loss: 1.7323 - accuracy: 0.4517\n",
            "Epoch 19/100\n",
            "568/568 - 10s - loss: 1.7305 - accuracy: 0.4537\n",
            "Epoch 20/100\n",
            "568/568 - 10s - loss: 1.7319 - accuracy: 0.4520\n",
            "Epoch 21/100\n",
            "568/568 - 10s - loss: 1.7268 - accuracy: 0.4514\n",
            "Epoch 22/100\n",
            "568/568 - 10s - loss: 1.7263 - accuracy: 0.4552\n",
            "Epoch 23/100\n",
            "568/568 - 10s - loss: 1.7223 - accuracy: 0.4554\n",
            "Epoch 24/100\n",
            "568/568 - 10s - loss: 1.7206 - accuracy: 0.4551\n",
            "Epoch 25/100\n",
            "568/568 - 10s - loss: 1.7188 - accuracy: 0.4557\n",
            "Epoch 26/100\n",
            "568/568 - 10s - loss: 1.7148 - accuracy: 0.4545\n",
            "Epoch 27/100\n",
            "568/568 - 10s - loss: 1.7132 - accuracy: 0.4566\n",
            "Epoch 28/100\n",
            "568/568 - 10s - loss: 1.7151 - accuracy: 0.4567\n",
            "Epoch 29/100\n",
            "568/568 - 10s - loss: 1.7116 - accuracy: 0.4574\n",
            "Epoch 30/100\n",
            "568/568 - 10s - loss: 1.7102 - accuracy: 0.4567\n",
            "Epoch 31/100\n",
            "568/568 - 10s - loss: 1.7095 - accuracy: 0.4572\n",
            "Epoch 32/100\n",
            "568/568 - 10s - loss: 1.7079 - accuracy: 0.4561\n",
            "Epoch 33/100\n",
            "568/568 - 10s - loss: 1.7066 - accuracy: 0.4568\n",
            "Epoch 34/100\n",
            "568/568 - 10s - loss: 1.7011 - accuracy: 0.4587\n",
            "Epoch 35/100\n",
            "568/568 - 10s - loss: 1.7018 - accuracy: 0.4597\n",
            "Epoch 36/100\n",
            "568/568 - 10s - loss: 1.7010 - accuracy: 0.4594\n",
            "Epoch 37/100\n",
            "568/568 - 10s - loss: 1.7046 - accuracy: 0.4580\n",
            "Epoch 38/100\n",
            "568/568 - 10s - loss: 1.6993 - accuracy: 0.4607\n",
            "Epoch 39/100\n",
            "568/568 - 10s - loss: 1.7000 - accuracy: 0.4627\n",
            "Epoch 40/100\n",
            "568/568 - 10s - loss: 1.6917 - accuracy: 0.4601\n",
            "Epoch 41/100\n",
            "568/568 - 10s - loss: 1.6936 - accuracy: 0.4609\n",
            "Epoch 42/100\n",
            "568/568 - 10s - loss: 1.6953 - accuracy: 0.4604\n",
            "Epoch 43/100\n",
            "568/568 - 10s - loss: 1.6916 - accuracy: 0.4605\n",
            "Epoch 44/100\n",
            "568/568 - 10s - loss: 1.6884 - accuracy: 0.4596\n",
            "Epoch 45/100\n",
            "568/568 - 10s - loss: 1.6881 - accuracy: 0.4631\n",
            "Epoch 46/100\n",
            "568/568 - 10s - loss: 1.6878 - accuracy: 0.4623\n",
            "Epoch 47/100\n",
            "568/568 - 10s - loss: 1.6876 - accuracy: 0.4615\n",
            "Epoch 48/100\n",
            "568/568 - 10s - loss: 1.6867 - accuracy: 0.4632\n",
            "Epoch 49/100\n",
            "568/568 - 10s - loss: 1.6792 - accuracy: 0.4656\n",
            "Epoch 50/100\n",
            "568/568 - 10s - loss: 1.6824 - accuracy: 0.4642\n",
            "Epoch 51/100\n",
            "568/568 - 10s - loss: 1.6845 - accuracy: 0.4618\n",
            "Epoch 52/100\n",
            "568/568 - 10s - loss: 1.6857 - accuracy: 0.4633\n",
            "Epoch 53/100\n",
            "568/568 - 10s - loss: 1.6823 - accuracy: 0.4648\n",
            "Epoch 54/100\n",
            "568/568 - 10s - loss: 1.6770 - accuracy: 0.4657\n",
            "Epoch 55/100\n",
            "568/568 - 10s - loss: 1.6791 - accuracy: 0.4655\n",
            "Epoch 56/100\n",
            "568/568 - 10s - loss: 1.6738 - accuracy: 0.4650\n",
            "Epoch 57/100\n",
            "568/568 - 10s - loss: 1.6743 - accuracy: 0.4681\n",
            "Epoch 58/100\n",
            "568/568 - 10s - loss: 1.6780 - accuracy: 0.4650\n",
            "Epoch 59/100\n",
            "568/568 - 10s - loss: 1.6742 - accuracy: 0.4659\n",
            "Epoch 60/100\n",
            "568/568 - 10s - loss: 1.6701 - accuracy: 0.4665\n",
            "Epoch 61/100\n",
            "568/568 - 10s - loss: 1.6726 - accuracy: 0.4679\n",
            "Epoch 62/100\n",
            "568/568 - 10s - loss: 1.6688 - accuracy: 0.4668\n",
            "Epoch 63/100\n",
            "568/568 - 10s - loss: 1.6687 - accuracy: 0.4652\n",
            "Epoch 64/100\n",
            "568/568 - 10s - loss: 1.6713 - accuracy: 0.4667\n",
            "Epoch 65/100\n",
            "568/568 - 10s - loss: 1.6669 - accuracy: 0.4676\n",
            "Epoch 66/100\n",
            "568/568 - 10s - loss: 1.6687 - accuracy: 0.4706\n",
            "Epoch 67/100\n",
            "568/568 - 10s - loss: 1.6654 - accuracy: 0.4695\n",
            "Epoch 68/100\n",
            "568/568 - 10s - loss: 1.6669 - accuracy: 0.4678\n",
            "Epoch 69/100\n",
            "568/568 - 10s - loss: 1.6653 - accuracy: 0.4703\n",
            "Epoch 70/100\n",
            "568/568 - 10s - loss: 1.6667 - accuracy: 0.4687\n",
            "Epoch 71/100\n",
            "568/568 - 10s - loss: 1.6645 - accuracy: 0.4673\n",
            "Epoch 72/100\n",
            "568/568 - 10s - loss: 1.6624 - accuracy: 0.4704\n",
            "Epoch 73/100\n",
            "568/568 - 10s - loss: 1.6626 - accuracy: 0.4727\n",
            "Epoch 74/100\n",
            "568/568 - 10s - loss: 1.6614 - accuracy: 0.4683\n",
            "Epoch 75/100\n",
            "568/568 - 10s - loss: 1.6660 - accuracy: 0.4708\n",
            "Epoch 76/100\n",
            "568/568 - 10s - loss: 1.6584 - accuracy: 0.4730\n",
            "Epoch 77/100\n",
            "568/568 - 10s - loss: 1.6629 - accuracy: 0.4719\n",
            "Epoch 78/100\n",
            "568/568 - 10s - loss: 1.6631 - accuracy: 0.4706\n",
            "Epoch 79/100\n",
            "568/568 - 10s - loss: 1.6598 - accuracy: 0.4724\n",
            "Epoch 80/100\n",
            "568/568 - 10s - loss: 1.6610 - accuracy: 0.4703\n",
            "Epoch 81/100\n",
            "568/568 - 10s - loss: 1.6572 - accuracy: 0.4732\n",
            "Epoch 82/100\n",
            "568/568 - 10s - loss: 1.6608 - accuracy: 0.4736\n",
            "Epoch 83/100\n",
            "568/568 - 10s - loss: 1.6613 - accuracy: 0.4716\n",
            "Epoch 84/100\n",
            "568/568 - 10s - loss: 1.6592 - accuracy: 0.4746\n",
            "Epoch 85/100\n",
            "568/568 - 10s - loss: 1.6554 - accuracy: 0.4719\n",
            "Epoch 86/100\n",
            "568/568 - 10s - loss: 1.6610 - accuracy: 0.4716\n",
            "Epoch 87/100\n",
            "568/568 - 10s - loss: 1.6562 - accuracy: 0.4727\n",
            "Epoch 88/100\n",
            "568/568 - 10s - loss: 1.6519 - accuracy: 0.4751\n",
            "Epoch 89/100\n",
            "568/568 - 10s - loss: 1.6539 - accuracy: 0.4737\n",
            "Epoch 90/100\n",
            "568/568 - 10s - loss: 1.6526 - accuracy: 0.4741\n",
            "Epoch 91/100\n",
            "568/568 - 10s - loss: 1.6528 - accuracy: 0.4751\n",
            "Epoch 92/100\n",
            "568/568 - 10s - loss: 1.6529 - accuracy: 0.4741\n",
            "Epoch 93/100\n",
            "568/568 - 10s - loss: 1.6544 - accuracy: 0.4751\n",
            "Epoch 94/100\n",
            "568/568 - 10s - loss: 1.6491 - accuracy: 0.4709\n",
            "Epoch 95/100\n",
            "568/568 - 10s - loss: 1.6527 - accuracy: 0.4729\n",
            "Epoch 96/100\n",
            "568/568 - 10s - loss: 1.6493 - accuracy: 0.4761\n",
            "Epoch 97/100\n",
            "568/568 - 10s - loss: 1.6518 - accuracy: 0.4733\n",
            "Epoch 98/100\n",
            "568/568 - 10s - loss: 1.6459 - accuracy: 0.4784\n",
            "Epoch 99/100\n",
            "568/568 - 10s - loss: 1.6484 - accuracy: 0.4743\n",
            "Epoch 100/100\n",
            "568/568 - 10s - loss: 1.6514 - accuracy: 0.4749\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6w0QloFS8lZ"
      },
      "source": [
        "print(cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABneoYnn_Qgi"
      },
      "source": [
        "###LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWYh05GmyFNb",
        "outputId": "e900ed59-173c-48bd-e249-8f403ac55ba0"
      },
      "source": [
        "def create_rnn_lstm():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((50, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(11, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_lstm()\n",
        "lstm = {}\n",
        "lstm = DNN_model(classifier,train_seq_x,onehot_train_y,test_seq_x,y_test)\n",
        "\n",
        "model['LSTM'] = lstm"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "568/568 - 26s - loss: 1.9502\n",
            "Epoch 2/100\n",
            "568/568 - 20s - loss: 1.8985\n",
            "Epoch 3/100\n",
            "568/568 - 21s - loss: 1.8780\n",
            "Epoch 4/100\n",
            "568/568 - 21s - loss: 1.8657\n",
            "Epoch 5/100\n",
            "568/568 - 20s - loss: 1.8541\n",
            "Epoch 6/100\n",
            "568/568 - 21s - loss: 1.8430\n",
            "Epoch 7/100\n",
            "568/568 - 20s - loss: 1.8338\n",
            "Epoch 8/100\n",
            "568/568 - 20s - loss: 1.8214\n",
            "Epoch 9/100\n",
            "568/568 - 21s - loss: 1.8087\n",
            "Epoch 10/100\n",
            "568/568 - 21s - loss: 1.8038\n",
            "Epoch 11/100\n",
            "568/568 - 21s - loss: 1.7956\n",
            "Epoch 12/100\n",
            "568/568 - 21s - loss: 1.7855\n",
            "Epoch 13/100\n",
            "568/568 - 21s - loss: 1.7714\n",
            "Epoch 14/100\n",
            "568/568 - 21s - loss: 1.7712\n",
            "Epoch 15/100\n",
            "568/568 - 21s - loss: 1.7610\n",
            "Epoch 16/100\n",
            "568/568 - 20s - loss: 1.7617\n",
            "Epoch 17/100\n",
            "568/568 - 20s - loss: 1.7460\n",
            "Epoch 18/100\n",
            "568/568 - 21s - loss: 1.7408\n",
            "Epoch 19/100\n",
            "568/568 - 21s - loss: 1.7328\n",
            "Epoch 20/100\n",
            "568/568 - 21s - loss: 1.7263\n",
            "Epoch 21/100\n",
            "568/568 - 21s - loss: 1.7158\n",
            "Epoch 22/100\n",
            "568/568 - 20s - loss: 1.7153\n",
            "Epoch 23/100\n",
            "568/568 - 20s - loss: 1.7011\n",
            "Epoch 24/100\n",
            "568/568 - 20s - loss: 1.6994\n",
            "Epoch 25/100\n",
            "568/568 - 21s - loss: 1.6926\n",
            "Epoch 26/100\n",
            "568/568 - 21s - loss: 1.6869\n",
            "Epoch 27/100\n",
            "568/568 - 21s - loss: 1.6812\n",
            "Epoch 28/100\n",
            "568/568 - 21s - loss: 1.6788\n",
            "Epoch 29/100\n",
            "568/568 - 20s - loss: 1.6656\n",
            "Epoch 30/100\n",
            "568/568 - 20s - loss: 1.6672\n",
            "Epoch 31/100\n",
            "568/568 - 21s - loss: 1.6575\n",
            "Epoch 32/100\n",
            "568/568 - 21s - loss: 1.6519\n",
            "Epoch 33/100\n",
            "568/568 - 20s - loss: 1.6505\n",
            "Epoch 34/100\n",
            "568/568 - 21s - loss: 1.6436\n",
            "Epoch 35/100\n",
            "568/568 - 21s - loss: 1.6414\n",
            "Epoch 36/100\n",
            "568/568 - 20s - loss: 1.6355\n",
            "Epoch 37/100\n",
            "568/568 - 20s - loss: 1.6245\n",
            "Epoch 38/100\n",
            "568/568 - 21s - loss: 1.6277\n",
            "Epoch 39/100\n",
            "568/568 - 20s - loss: 1.6170\n",
            "Epoch 40/100\n",
            "568/568 - 20s - loss: 1.6113\n",
            "Epoch 41/100\n",
            "568/568 - 21s - loss: 1.6124\n",
            "Epoch 42/100\n",
            "568/568 - 21s - loss: 1.6025\n",
            "Epoch 43/100\n",
            "568/568 - 21s - loss: 1.5952\n",
            "Epoch 44/100\n",
            "568/568 - 21s - loss: 1.5897\n",
            "Epoch 45/100\n",
            "568/568 - 20s - loss: 1.5825\n",
            "Epoch 46/100\n",
            "568/568 - 20s - loss: 1.5756\n",
            "Epoch 47/100\n",
            "568/568 - 20s - loss: 1.5765\n",
            "Epoch 48/100\n",
            "568/568 - 21s - loss: 1.5639\n",
            "Epoch 49/100\n",
            "568/568 - 20s - loss: 1.5560\n",
            "Epoch 50/100\n",
            "568/568 - 20s - loss: 1.5610\n",
            "Epoch 51/100\n",
            "568/568 - 21s - loss: 1.5501\n",
            "Epoch 52/100\n",
            "568/568 - 21s - loss: 1.5485\n",
            "Epoch 53/100\n",
            "568/568 - 21s - loss: 1.5381\n",
            "Epoch 54/100\n",
            "568/568 - 21s - loss: 1.5306\n",
            "Epoch 55/100\n",
            "568/568 - 21s - loss: 1.5228\n",
            "Epoch 56/100\n",
            "568/568 - 21s - loss: 1.5155\n",
            "Epoch 57/100\n",
            "568/568 - 21s - loss: 1.5147\n",
            "Epoch 58/100\n",
            "568/568 - 21s - loss: 1.4994\n",
            "Epoch 59/100\n",
            "568/568 - 20s - loss: 1.5028\n",
            "Epoch 60/100\n",
            "568/568 - 20s - loss: 1.4958\n",
            "Epoch 61/100\n",
            "568/568 - 21s - loss: 1.4894\n",
            "Epoch 62/100\n",
            "568/568 - 20s - loss: 1.4834\n",
            "Epoch 63/100\n",
            "568/568 - 21s - loss: 1.4764\n",
            "Epoch 64/100\n",
            "568/568 - 21s - loss: 1.4629\n",
            "Epoch 65/100\n",
            "568/568 - 20s - loss: 1.4584\n",
            "Epoch 66/100\n",
            "568/568 - 20s - loss: 1.4616\n",
            "Epoch 67/100\n",
            "568/568 - 20s - loss: 1.4608\n",
            "Epoch 68/100\n",
            "568/568 - 20s - loss: 1.4492\n",
            "Epoch 69/100\n",
            "568/568 - 20s - loss: 1.4493\n",
            "Epoch 70/100\n",
            "568/568 - 20s - loss: 1.4395\n",
            "Epoch 71/100\n",
            "568/568 - 20s - loss: 1.4286\n",
            "Epoch 72/100\n",
            "568/568 - 20s - loss: 1.4247\n",
            "Epoch 73/100\n",
            "568/568 - 21s - loss: 1.4229\n",
            "Epoch 74/100\n",
            "568/568 - 21s - loss: 1.4058\n",
            "Epoch 75/100\n",
            "568/568 - 20s - loss: 1.4158\n",
            "Epoch 76/100\n",
            "568/568 - 20s - loss: 1.4072\n",
            "Epoch 77/100\n",
            "568/568 - 20s - loss: 1.3960\n",
            "Epoch 78/100\n",
            "568/568 - 21s - loss: 1.3911\n",
            "Epoch 79/100\n",
            "568/568 - 20s - loss: 1.3841\n",
            "Epoch 80/100\n",
            "568/568 - 21s - loss: 1.3837\n",
            "Epoch 81/100\n",
            "568/568 - 20s - loss: 1.3855\n",
            "Epoch 82/100\n",
            "568/568 - 21s - loss: 1.3747\n",
            "Epoch 83/100\n",
            "568/568 - 21s - loss: 1.3724\n",
            "Epoch 84/100\n",
            "568/568 - 20s - loss: 1.3578\n",
            "Epoch 85/100\n",
            "568/568 - 20s - loss: 1.3623\n",
            "Epoch 86/100\n",
            "568/568 - 20s - loss: 1.3514\n",
            "Epoch 87/100\n",
            "568/568 - 20s - loss: 1.3450\n",
            "Epoch 88/100\n",
            "568/568 - 22s - loss: 1.3462\n",
            "Epoch 89/100\n",
            "568/568 - 21s - loss: 1.3389\n",
            "Epoch 90/100\n",
            "568/568 - 21s - loss: 1.3407\n",
            "Epoch 91/100\n",
            "568/568 - 20s - loss: 1.3322\n",
            "Epoch 92/100\n",
            "568/568 - 20s - loss: 1.3290\n",
            "Epoch 93/100\n",
            "568/568 - 21s - loss: 1.3298\n",
            "Epoch 94/100\n",
            "568/568 - 21s - loss: 1.3124\n",
            "Epoch 95/100\n",
            "568/568 - 20s - loss: 1.3140\n",
            "Epoch 96/100\n",
            "568/568 - 20s - loss: 1.3181\n",
            "Epoch 97/100\n",
            "568/568 - 21s - loss: 1.3077\n",
            "Epoch 98/100\n",
            "568/568 - 20s - loss: 1.3057\n",
            "Epoch 99/100\n",
            "568/568 - 20s - loss: 1.3086\n",
            "Epoch 100/100\n",
            "568/568 - 21s - loss: 1.2982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myLYEP1AGE9r",
        "outputId": "d1900727-2cf8-4534-b3b5-ccec3adec51e"
      },
      "source": [
        "print(lstm['accuracy'])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.41554723629156576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFcrz-lN_YHC"
      },
      "source": [
        "### Gated Recurrent Unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp2hWrWkih3S",
        "outputId": "815f50d4-73ec-489a-9391-253611d1d3f2"
      },
      "source": [
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((50, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(11, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics= ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_gru()\n",
        "rnn_gru = {}\n",
        "rnn_gru = DNN_model(classifier,train_seq_x,onehot_train_y,test_seq_x,y_test)\n",
        "model['GATED RNN'] = rnn_gru"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "568/568 - 23s - loss: 1.9694 - accuracy: 0.4013\n",
            "Epoch 2/100\n",
            "568/568 - 19s - loss: 1.8943 - accuracy: 0.4155\n",
            "Epoch 3/100\n",
            "568/568 - 19s - loss: 1.8797 - accuracy: 0.4195\n",
            "Epoch 4/100\n",
            "568/568 - 19s - loss: 1.8668 - accuracy: 0.4202\n",
            "Epoch 5/100\n",
            "568/568 - 19s - loss: 1.8560 - accuracy: 0.4222\n",
            "Epoch 6/100\n",
            "568/568 - 19s - loss: 1.8396 - accuracy: 0.4269\n",
            "Epoch 7/100\n",
            "568/568 - 19s - loss: 1.8165 - accuracy: 0.4322\n",
            "Epoch 8/100\n",
            "568/568 - 19s - loss: 1.8072 - accuracy: 0.4347\n",
            "Epoch 9/100\n",
            "568/568 - 19s - loss: 1.7957 - accuracy: 0.4351\n",
            "Epoch 10/100\n",
            "568/568 - 19s - loss: 1.7796 - accuracy: 0.4397\n",
            "Epoch 11/100\n",
            "568/568 - 19s - loss: 1.7708 - accuracy: 0.4421\n",
            "Epoch 12/100\n",
            "568/568 - 19s - loss: 1.7578 - accuracy: 0.4432\n",
            "Epoch 13/100\n",
            "568/568 - 19s - loss: 1.7471 - accuracy: 0.4482\n",
            "Epoch 14/100\n",
            "568/568 - 19s - loss: 1.7381 - accuracy: 0.4531\n",
            "Epoch 15/100\n",
            "568/568 - 19s - loss: 1.7277 - accuracy: 0.4532\n",
            "Epoch 16/100\n",
            "568/568 - 19s - loss: 1.7269 - accuracy: 0.4549\n",
            "Epoch 17/100\n",
            "568/568 - 19s - loss: 1.7149 - accuracy: 0.4570\n",
            "Epoch 18/100\n",
            "568/568 - 19s - loss: 1.7092 - accuracy: 0.4608\n",
            "Epoch 19/100\n",
            "568/568 - 19s - loss: 1.6993 - accuracy: 0.4595\n",
            "Epoch 20/100\n",
            "568/568 - 20s - loss: 1.6968 - accuracy: 0.4617\n",
            "Epoch 21/100\n",
            "568/568 - 19s - loss: 1.6945 - accuracy: 0.4656\n",
            "Epoch 22/100\n",
            "568/568 - 19s - loss: 1.6888 - accuracy: 0.4659\n",
            "Epoch 23/100\n",
            "568/568 - 19s - loss: 1.6850 - accuracy: 0.4671\n",
            "Epoch 24/100\n",
            "568/568 - 19s - loss: 1.7155 - accuracy: 0.4602\n",
            "Epoch 25/100\n",
            "568/568 - 19s - loss: 1.6763 - accuracy: 0.4689\n",
            "Epoch 26/100\n",
            "568/568 - 19s - loss: 1.6745 - accuracy: 0.4701\n",
            "Epoch 27/100\n",
            "568/568 - 19s - loss: 1.6637 - accuracy: 0.4714\n",
            "Epoch 28/100\n",
            "568/568 - 19s - loss: 1.6617 - accuracy: 0.4724\n",
            "Epoch 29/100\n",
            "568/568 - 19s - loss: 1.6585 - accuracy: 0.4708\n",
            "Epoch 30/100\n",
            "568/568 - 19s - loss: 1.6507 - accuracy: 0.4746\n",
            "Epoch 31/100\n",
            "568/568 - 19s - loss: 1.6454 - accuracy: 0.4762\n",
            "Epoch 32/100\n",
            "568/568 - 19s - loss: 1.6388 - accuracy: 0.4772\n",
            "Epoch 33/100\n",
            "568/568 - 19s - loss: 1.6390 - accuracy: 0.4811\n",
            "Epoch 34/100\n",
            "568/568 - 19s - loss: 1.6281 - accuracy: 0.4792\n",
            "Epoch 35/100\n",
            "568/568 - 19s - loss: 1.6255 - accuracy: 0.4827\n",
            "Epoch 36/100\n",
            "568/568 - 19s - loss: 1.6207 - accuracy: 0.4811\n",
            "Epoch 37/100\n",
            "568/568 - 19s - loss: 1.6159 - accuracy: 0.4815\n",
            "Epoch 38/100\n",
            "568/568 - 19s - loss: 1.6044 - accuracy: 0.4850\n",
            "Epoch 39/100\n",
            "568/568 - 19s - loss: 1.6078 - accuracy: 0.4834\n",
            "Epoch 40/100\n",
            "568/568 - 19s - loss: 1.5918 - accuracy: 0.4879\n",
            "Epoch 41/100\n",
            "568/568 - 19s - loss: 1.5866 - accuracy: 0.4899\n",
            "Epoch 42/100\n",
            "568/568 - 19s - loss: 1.5883 - accuracy: 0.4906\n",
            "Epoch 43/100\n",
            "568/568 - 19s - loss: 1.5815 - accuracy: 0.4926\n",
            "Epoch 44/100\n",
            "568/568 - 19s - loss: 1.5757 - accuracy: 0.4932\n",
            "Epoch 45/100\n",
            "568/568 - 19s - loss: 1.5693 - accuracy: 0.4948\n",
            "Epoch 46/100\n",
            "568/568 - 19s - loss: 1.5622 - accuracy: 0.4994\n",
            "Epoch 47/100\n",
            "568/568 - 19s - loss: 1.5589 - accuracy: 0.5001\n",
            "Epoch 48/100\n",
            "568/568 - 19s - loss: 1.5494 - accuracy: 0.4994\n",
            "Epoch 49/100\n",
            "568/568 - 19s - loss: 1.5489 - accuracy: 0.5014\n",
            "Epoch 50/100\n",
            "568/568 - 19s - loss: 1.5343 - accuracy: 0.5038\n",
            "Epoch 51/100\n",
            "568/568 - 19s - loss: 1.5433 - accuracy: 0.5030\n",
            "Epoch 52/100\n",
            "568/568 - 19s - loss: 1.5339 - accuracy: 0.5038\n",
            "Epoch 53/100\n",
            "568/568 - 19s - loss: 1.5258 - accuracy: 0.5053\n",
            "Epoch 54/100\n",
            "568/568 - 19s - loss: 1.5249 - accuracy: 0.5057\n",
            "Epoch 55/100\n",
            "568/568 - 19s - loss: 1.5146 - accuracy: 0.5100\n",
            "Epoch 56/100\n",
            "568/568 - 19s - loss: 1.5105 - accuracy: 0.5130\n",
            "Epoch 57/100\n",
            "568/568 - 19s - loss: 1.5005 - accuracy: 0.5161\n",
            "Epoch 58/100\n",
            "568/568 - 19s - loss: 1.4998 - accuracy: 0.5124\n",
            "Epoch 59/100\n",
            "568/568 - 19s - loss: 1.4901 - accuracy: 0.5176\n",
            "Epoch 60/100\n",
            "568/568 - 19s - loss: 1.4885 - accuracy: 0.5154\n",
            "Epoch 61/100\n",
            "568/568 - 19s - loss: 1.4812 - accuracy: 0.5202\n",
            "Epoch 62/100\n",
            "568/568 - 19s - loss: 1.4755 - accuracy: 0.5220\n",
            "Epoch 63/100\n",
            "568/568 - 19s - loss: 1.4693 - accuracy: 0.5218\n",
            "Epoch 64/100\n",
            "568/568 - 19s - loss: 1.4685 - accuracy: 0.5265\n",
            "Epoch 65/100\n",
            "568/568 - 19s - loss: 1.4632 - accuracy: 0.5248\n",
            "Epoch 66/100\n",
            "568/568 - 19s - loss: 1.4596 - accuracy: 0.5253\n",
            "Epoch 67/100\n",
            "568/568 - 19s - loss: 1.4555 - accuracy: 0.5239\n",
            "Epoch 68/100\n",
            "568/568 - 19s - loss: 1.4525 - accuracy: 0.5262\n",
            "Epoch 69/100\n",
            "568/568 - 19s - loss: 1.4476 - accuracy: 0.5290\n",
            "Epoch 70/100\n",
            "568/568 - 19s - loss: 1.4405 - accuracy: 0.5312\n",
            "Epoch 71/100\n",
            "568/568 - 19s - loss: 1.4385 - accuracy: 0.5309\n",
            "Epoch 72/100\n",
            "568/568 - 19s - loss: 1.4331 - accuracy: 0.5319\n",
            "Epoch 73/100\n",
            "568/568 - 19s - loss: 1.4299 - accuracy: 0.5323\n",
            "Epoch 74/100\n",
            "568/568 - 19s - loss: 1.4200 - accuracy: 0.5397\n",
            "Epoch 75/100\n",
            "568/568 - 19s - loss: 1.4196 - accuracy: 0.5362\n",
            "Epoch 76/100\n",
            "568/568 - 19s - loss: 1.4163 - accuracy: 0.5422\n",
            "Epoch 77/100\n",
            "568/568 - 19s - loss: 1.4180 - accuracy: 0.5378\n",
            "Epoch 78/100\n",
            "568/568 - 19s - loss: 1.4173 - accuracy: 0.5361\n",
            "Epoch 79/100\n",
            "568/568 - 19s - loss: 1.4050 - accuracy: 0.5416\n",
            "Epoch 80/100\n",
            "568/568 - 20s - loss: 1.3991 - accuracy: 0.5421\n",
            "Epoch 81/100\n",
            "568/568 - 20s - loss: 1.4025 - accuracy: 0.5430\n",
            "Epoch 82/100\n",
            "568/568 - 19s - loss: 1.3885 - accuracy: 0.5463\n",
            "Epoch 83/100\n",
            "568/568 - 19s - loss: 1.3834 - accuracy: 0.5472\n",
            "Epoch 84/100\n",
            "568/568 - 19s - loss: 1.3846 - accuracy: 0.5465\n",
            "Epoch 85/100\n",
            "568/568 - 19s - loss: 1.3852 - accuracy: 0.5468\n",
            "Epoch 86/100\n",
            "568/568 - 19s - loss: 1.3765 - accuracy: 0.5479\n",
            "Epoch 87/100\n",
            "568/568 - 19s - loss: 1.3751 - accuracy: 0.5536\n",
            "Epoch 88/100\n",
            "568/568 - 19s - loss: 1.3688 - accuracy: 0.5532\n",
            "Epoch 89/100\n",
            "568/568 - 19s - loss: 1.3689 - accuracy: 0.5508\n",
            "Epoch 90/100\n",
            "568/568 - 19s - loss: 1.3662 - accuracy: 0.5530\n",
            "Epoch 91/100\n",
            "568/568 - 19s - loss: 1.3665 - accuracy: 0.5516\n",
            "Epoch 92/100\n",
            "568/568 - 19s - loss: 1.3540 - accuracy: 0.5555\n",
            "Epoch 93/100\n",
            "568/568 - 19s - loss: 1.3549 - accuracy: 0.5560\n",
            "Epoch 94/100\n",
            "568/568 - 19s - loss: 1.3516 - accuracy: 0.5585\n",
            "Epoch 95/100\n",
            "568/568 - 19s - loss: 1.3512 - accuracy: 0.5570\n",
            "Epoch 96/100\n",
            "568/568 - 19s - loss: 1.3460 - accuracy: 0.5596\n",
            "Epoch 97/100\n",
            "568/568 - 19s - loss: 1.3616 - accuracy: 0.5552\n",
            "Epoch 98/100\n",
            "568/568 - 19s - loss: 1.3362 - accuracy: 0.5596\n",
            "Epoch 99/100\n",
            "568/568 - 19s - loss: 1.3374 - accuracy: 0.5583\n",
            "Epoch 100/100\n",
            "568/568 - 19s - loss: 1.3278 - accuracy: 0.5633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ZXdECW_lkK"
      },
      "source": [
        "### Recurrent CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WM5gptYjGM6"
      },
      "source": [
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((50, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(11, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rcnn()\n",
        "rcnn = {}\n",
        "rcnn = DNN_model(classifier,train_seq_x,onehot_train_y,test_seq_x,y_test)\n",
        "\n",
        "model['RCNN'] = rcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUA1_VlZYlfG",
        "outputId": "6c7d5944-9c42-4b96-a020-6039daa94cf8"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "vocab_size=len(word_index)+1\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=50,trainable=False))\n",
        "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(25, activation='relu'))\n",
        "model.add(Dense(11, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(train_seq_x, onehot_train_y, epochs=30, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(test_seq_x, onehot_test_y, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 50, 100)           1141000   \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 43, 32)            25632     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 21, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 672)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 25)                16825     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 11)                286       \n",
            "=================================================================\n",
            "Total params: 1,183,743\n",
            "Trainable params: 42,743\n",
            "Non-trainable params: 1,141,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "568/568 - 7s - loss: 2.0149 - accuracy: 0.3748\n",
            "Epoch 2/30\n",
            "568/568 - 6s - loss: 1.9461 - accuracy: 0.3938\n",
            "Epoch 3/30\n",
            "568/568 - 6s - loss: 1.8934 - accuracy: 0.4074\n",
            "Epoch 4/30\n",
            "568/568 - 6s - loss: 1.8280 - accuracy: 0.4291\n",
            "Epoch 5/30\n",
            "568/568 - 6s - loss: 1.7547 - accuracy: 0.4548\n",
            "Epoch 6/30\n",
            "568/568 - 6s - loss: 1.6907 - accuracy: 0.4727\n",
            "Epoch 7/30\n",
            "568/568 - 6s - loss: 1.6293 - accuracy: 0.4910\n",
            "Epoch 8/30\n",
            "568/568 - 6s - loss: 1.5757 - accuracy: 0.5093\n",
            "Epoch 9/30\n",
            "568/568 - 6s - loss: 1.5182 - accuracy: 0.5238\n",
            "Epoch 10/30\n",
            "568/568 - 6s - loss: 1.4649 - accuracy: 0.5421\n",
            "Epoch 11/30\n",
            "568/568 - 6s - loss: 1.4151 - accuracy: 0.5558\n",
            "Epoch 12/30\n",
            "568/568 - 6s - loss: 1.3664 - accuracy: 0.5744\n",
            "Epoch 13/30\n",
            "568/568 - 6s - loss: 1.3179 - accuracy: 0.5887\n",
            "Epoch 14/30\n",
            "568/568 - 6s - loss: 1.2741 - accuracy: 0.6010\n",
            "Epoch 15/30\n",
            "568/568 - 6s - loss: 1.2347 - accuracy: 0.6123\n",
            "Epoch 16/30\n",
            "568/568 - 6s - loss: 1.1959 - accuracy: 0.6258\n",
            "Epoch 17/30\n",
            "568/568 - 6s - loss: 1.1585 - accuracy: 0.6393\n",
            "Epoch 18/30\n",
            "568/568 - 6s - loss: 1.1264 - accuracy: 0.6466\n",
            "Epoch 19/30\n",
            "568/568 - 7s - loss: 1.0930 - accuracy: 0.6590\n",
            "Epoch 20/30\n",
            "568/568 - 7s - loss: 1.0634 - accuracy: 0.6669\n",
            "Epoch 21/30\n",
            "568/568 - 7s - loss: 1.0350 - accuracy: 0.6759\n",
            "Epoch 22/30\n",
            "568/568 - 7s - loss: 1.0061 - accuracy: 0.6837\n",
            "Epoch 23/30\n",
            "568/568 - 7s - loss: 0.9799 - accuracy: 0.6915\n",
            "Epoch 24/30\n",
            "568/568 - 7s - loss: 0.9582 - accuracy: 0.6999\n",
            "Epoch 25/30\n",
            "568/568 - 7s - loss: 0.9311 - accuracy: 0.7097\n",
            "Epoch 26/30\n",
            "568/568 - 7s - loss: 0.9092 - accuracy: 0.7160\n",
            "Epoch 27/30\n",
            "568/568 - 7s - loss: 0.8874 - accuracy: 0.7247\n",
            "Epoch 28/30\n",
            "568/568 - 7s - loss: 0.8678 - accuracy: 0.7302\n",
            "Epoch 29/30\n",
            "568/568 - 7s - loss: 0.8479 - accuracy: 0.7369\n",
            "Epoch 30/30\n",
            "568/568 - 7s - loss: 0.8290 - accuracy: 0.7423\n",
            "Test Accuracy: 41.070250\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}